/home/he.1773/miniconda3-23.9.0/envs/confMILE
92499
a100-05
Tue Dec 19 23:20:11 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |
| N/A   25C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:C1:00.0 Off |                    0 |
| N/A   24C    P0              31W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
/usr/bin/nvidia-smi
2023-12-19 23:20:53.362310: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:26:20 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.1, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:26:24 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:26:24 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:26:24 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:26:24 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:26:24 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:26:24 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:26:24 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:26:24 INFO     	| Interval 0 (graph coarsening) time 0.24375 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:26:24 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:26:36 INFO     	| Interval 1 (embedding) time 11.71352 s
2023-12-19 23:26:36 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:26:37 INFO     initial_embed: (1518, 128)
2023-12-19 23:26:37 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.25476202368736267, MSE Loss: 0.14712491631507874, ACC loss: 1.2234960794448853
Epoch 200, Loss: 0.16242697834968567, MSE Loss: 0.07571449130773544, ACC loss: 0.9428392648696899
2023-12-19 23:26:47 INFO     		| Interval 1 (training the model) time 11.67101 s
2023-12-19 23:26:47 INFO     			Refinement at level 3 completed.
2023-12-19 23:26:47 INFO     			Refinement at level 2 completed.
2023-12-19 23:26:47 INFO     			Refinement at level 1 completed.
2023-12-19 23:26:47 INFO     		| Interval 2 (refinement) time 0.05733 s
2023-12-19 23:26:47 INFO     	| Interval 2 (refinement training and applying) time 11.72973 s
2023-12-19 23:26:47 INFO     | Time for this section (main program): 23.68701 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.137983322143555
The empirical coverage with aps is: 0.9181197881698608
The empirical efficiency with daps is: 5.139499664306641
The empirical coverage with daps is: 0.9120545983314514
2023-12-19 23:27:05.725356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:27:14 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.1, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:27:14 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:27:14 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:27:14 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:27:14 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:27:14 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:27:14 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:27:14 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:27:14 INFO     	| Interval 0 (graph coarsening) time 0.24459 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:27:14 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:27:25 INFO     	| Interval 1 (embedding) time 10.79292 s
2023-12-19 23:27:25 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:27:25 INFO     initial_embed: (1518, 128)
2023-12-19 23:27:25 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.21817302703857422, MSE Loss: 0.0993448793888092, ACC loss: 1.2876265048980713
Epoch 200, Loss: 0.16194668412208557, MSE Loss: 0.05815056338906288, ACC loss: 1.09611177444458
2023-12-19 23:27:33 INFO     		| Interval 1 (training the model) time 8.20904 s
2023-12-19 23:27:33 INFO     			Refinement at level 3 completed.
2023-12-19 23:27:33 INFO     			Refinement at level 2 completed.
2023-12-19 23:27:33 INFO     			Refinement at level 1 completed.
2023-12-19 23:27:33 INFO     		| Interval 2 (refinement) time 0.05322 s
2023-12-19 23:27:33 INFO     	| Interval 2 (refinement training and applying) time 8.26355 s
2023-12-19 23:27:33 INFO     | Time for this section (main program): 19.30106 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.9545111656188965
The empirical coverage with aps is: 0.905231237411499
The empirical efficiency with daps is: 4.869598388671875
The empirical coverage with daps is: 0.904473066329956
2023-12-19 23:27:38.291472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:27:46 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.1, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:27:47 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:27:47 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:27:47 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:27:47 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:27:47 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:27:47 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:27:47 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:27:47 INFO     	| Interval 0 (graph coarsening) time 0.24476 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:27:47 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:27:58 INFO     	| Interval 1 (embedding) time 11.17695 s
2023-12-19 23:27:58 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:27:58 INFO     initial_embed: (1518, 128)
2023-12-19 23:27:58 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.23596805334091187, MSE Loss: 0.10877984762191772, ACC loss: 1.380661964416504
Epoch 200, Loss: 0.1643015742301941, MSE Loss: 0.05519215762615204, ACC loss: 1.1462863683700562
2023-12-19 23:28:06 INFO     		| Interval 1 (training the model) time 8.24707 s
2023-12-19 23:28:06 INFO     			Refinement at level 3 completed.
2023-12-19 23:28:06 INFO     			Refinement at level 2 completed.
2023-12-19 23:28:06 INFO     			Refinement at level 1 completed.
2023-12-19 23:28:06 INFO     		| Interval 2 (refinement) time 0.05376 s
2023-12-19 23:28:06 INFO     	| Interval 2 (refinement training and applying) time 8.30257 s
2023-12-19 23:28:06 INFO     | Time for this section (main program): 19.72428 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.81349515914917
The empirical coverage with aps is: 0.8786959648132324
The empirical efficiency with daps is: 4.8377556800842285
The empirical coverage with daps is: 0.8794541358947754
2023-12-19 23:28:10.223888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:28:19 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.1, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:28:19 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:28:19 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:28:19 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:28:19 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:28:19 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:28:19 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:28:19 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:28:19 INFO     	| Interval 0 (graph coarsening) time 0.24598 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:28:19 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:28:29 INFO     	| Interval 1 (embedding) time 10.11648 s
2023-12-19 23:28:29 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:28:29 INFO     initial_embed: (1518, 128)
2023-12-19 23:28:29 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.4623388648033142, MSE Loss: 0.3280930817127228, ACC loss: 1.6705509424209595
Epoch 200, Loss: 0.2923833727836609, MSE Loss: 0.1514982134103775, ACC loss: 1.560349941253662
2023-12-19 23:28:38 INFO     		| Interval 1 (training the model) time 8.84187 s
2023-12-19 23:28:38 INFO     			Refinement at level 3 completed.
2023-12-19 23:28:38 INFO     			Refinement at level 2 completed.
2023-12-19 23:28:38 INFO     			Refinement at level 1 completed.
2023-12-19 23:28:38 INFO     		| Interval 2 (refinement) time 0.06662 s
2023-12-19 23:28:38 INFO     	| Interval 2 (refinement training and applying) time 8.91052 s
2023-12-19 23:28:38 INFO     | Time for this section (main program): 19.27297 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.73616361618042
The empirical coverage with aps is: 0.8931008577346802
The empirical efficiency with daps is: 4.755117416381836
The empirical coverage with daps is: 0.8961334228515625
2023-12-19 23:28:43.469270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:28:52 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.3, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:28:52 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:28:52 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:28:52 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:28:52 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:28:52 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:28:52 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:28:52 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:28:52 INFO     	| Interval 0 (graph coarsening) time 0.24386 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:28:52 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:29:02 INFO     	| Interval 1 (embedding) time 10.43592 s
2023-12-19 23:29:02 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:29:03 INFO     initial_embed: (1518, 128)
2023-12-19 23:29:03 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.4401579201221466, MSE Loss: 0.19317913055419922, ACC loss: 1.0164417028427124
Epoch 200, Loss: 0.33353710174560547, MSE Loss: 0.14112207293510437, ACC loss: 0.782505452632904
2023-12-19 23:29:11 INFO     		| Interval 1 (training the model) time 8.52932 s
2023-12-19 23:29:11 INFO     			Refinement at level 3 completed.
2023-12-19 23:29:11 INFO     			Refinement at level 2 completed.
2023-12-19 23:29:11 INFO     			Refinement at level 1 completed.
2023-12-19 23:29:11 INFO     		| Interval 2 (refinement) time 0.06358 s
2023-12-19 23:29:11 INFO     	| Interval 2 (refinement training and applying) time 8.59451 s
2023-12-19 23:29:11 INFO     | Time for this section (main program): 19.27429 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.154662609100342
The empirical coverage with aps is: 0.8968915939331055
The empirical efficiency with daps is: 5.190295696258545
The empirical coverage with daps is: 0.8991660475730896
2023-12-19 23:29:14.829149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:29:23 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.3, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:29:23 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:29:23 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:29:24 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:29:24 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:29:24 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:29:24 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:29:24 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:29:24 INFO     	| Interval 0 (graph coarsening) time 0.24243 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:29:24 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:29:35 INFO     	| Interval 1 (embedding) time 11.83427 s
2023-12-19 23:29:35 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:29:35 INFO     initial_embed: (1518, 128)
2023-12-19 23:29:35 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.4197094738483429, MSE Loss: 0.14570343494415283, ACC loss: 1.0590568780899048
Epoch 200, Loss: 0.3357495665550232, MSE Loss: 0.09965769201517105, ACC loss: 0.8866305351257324
2023-12-19 23:29:44 INFO     		| Interval 1 (training the model) time 8.18841 s
2023-12-19 23:29:44 INFO     			Refinement at level 3 completed.
2023-12-19 23:29:44 INFO     			Refinement at level 2 completed.
2023-12-19 23:29:44 INFO     			Refinement at level 1 completed.
2023-12-19 23:29:44 INFO     		| Interval 2 (refinement) time 0.05213 s
2023-12-19 23:29:44 INFO     	| Interval 2 (refinement training and applying) time 8.24241 s
2023-12-19 23:29:44 INFO     | Time for this section (main program): 20.31911 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.175890922546387
The empirical coverage with aps is: 0.8968915939331055
The empirical efficiency with daps is: 5.199393272399902
The empirical coverage with daps is: 0.8968915939331055
2023-12-19 23:29:46.878159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:29:55 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.3, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:29:55 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:29:55 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:29:55 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:29:55 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:29:55 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:29:55 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:29:55 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:29:55 INFO     	| Interval 0 (graph coarsening) time 0.24074 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:29:55 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:30:08 INFO     	| Interval 1 (embedding) time 12.13282 s
2023-12-19 23:30:08 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:30:08 INFO     initial_embed: (1518, 128)
2023-12-19 23:30:08 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.47421395778656006, MSE Loss: 0.13903100788593292, ACC loss: 1.2563074827194214
Epoch 200, Loss: 0.3711562156677246, MSE Loss: 0.10045868903398514, ACC loss: 1.0027837753295898
2023-12-19 23:30:17 INFO     		| Interval 1 (training the model) time 9.43027 s
2023-12-19 23:30:17 INFO     			Refinement at level 3 completed.
2023-12-19 23:30:17 INFO     			Refinement at level 2 completed.
2023-12-19 23:30:17 INFO     			Refinement at level 1 completed.
2023-12-19 23:30:17 INFO     		| Interval 2 (refinement) time 0.06494 s
2023-12-19 23:30:17 INFO     	| Interval 2 (refinement training and applying) time 9.49670 s
2023-12-19 23:30:17 INFO     | Time for this section (main program): 21.87026 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.062168121337891
The empirical coverage with aps is: 0.9166035056114197
The empirical efficiency with daps is: 5.0447306632995605
The empirical coverage with daps is: 0.9120545983314514
2023-12-19 23:30:20.772354: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:30:29 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.3, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:30:29 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:30:29 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:30:29 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:30:29 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:30:29 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:30:29 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:30:29 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:30:29 INFO     	| Interval 0 (graph coarsening) time 0.24356 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:30:29 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:30:41 INFO     	| Interval 1 (embedding) time 11.83660 s
2023-12-19 23:30:41 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:30:41 INFO     initial_embed: (1518, 128)
2023-12-19 23:30:41 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7236340641975403, MSE Loss: 0.34403476119041443, ACC loss: 1.6093658208847046
Epoch 200, Loss: 0.5581679344177246, MSE Loss: 0.173915833234787, ACC loss: 1.4547561407089233
2023-12-19 23:30:49 INFO     		| Interval 1 (training the model) time 8.07439 s
2023-12-19 23:30:49 INFO     			Refinement at level 3 completed.
2023-12-19 23:30:49 INFO     			Refinement at level 2 completed.
2023-12-19 23:30:49 INFO     			Refinement at level 1 completed.
2023-12-19 23:30:49 INFO     		| Interval 2 (refinement) time 0.05065 s
2023-12-19 23:30:49 INFO     	| Interval 2 (refinement training and applying) time 8.12698 s
2023-12-19 23:30:49 INFO     | Time for this section (main program): 20.20714 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.77862024307251
The empirical coverage with aps is: 0.8915845155715942
The empirical efficiency with daps is: 4.887794017791748
The empirical coverage with daps is: 0.8984078764915466
2023-12-19 23:30:52.502901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:31:01 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.5, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:31:01 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:31:01 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:31:01 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:31:01 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:31:01 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:31:01 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:31:01 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:31:01 INFO     	| Interval 0 (graph coarsening) time 0.24232 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:31:01 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:31:11 INFO     	| Interval 1 (embedding) time 10.35722 s
2023-12-19 23:31:11 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:31:11 INFO     initial_embed: (1518, 128)
2023-12-19 23:31:11 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.5928671360015869, MSE Loss: 0.276853084564209, ACC loss: 0.9088811874389648
Epoch 200, Loss: 0.3614678978919983, MSE Loss: 0.24974393844604492, ACC loss: 0.47319188714027405
2023-12-19 23:31:20 INFO     		| Interval 1 (training the model) time 8.41919 s
2023-12-19 23:31:20 INFO     			Refinement at level 3 completed.
2023-12-19 23:31:20 INFO     			Refinement at level 2 completed.
2023-12-19 23:31:20 INFO     			Refinement at level 1 completed.
2023-12-19 23:31:20 INFO     		| Interval 2 (refinement) time 0.04848 s
2023-12-19 23:31:20 INFO     	| Interval 2 (refinement training and applying) time 8.46933 s
2023-12-19 23:31:20 INFO     | Time for this section (main program): 19.06888 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.07733154296875
The empirical coverage with aps is: 0.8938589692115784
The empirical efficiency with daps is: 5.105382919311523
The empirical coverage with daps is: 0.8961334228515625
2023-12-19 23:31:23.500024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:31:32 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.5, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:31:32 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:31:32 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:31:32 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:31:32 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:31:32 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:31:32 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:31:32 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:31:32 INFO     	| Interval 0 (graph coarsening) time 0.24282 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:31:32 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:31:42 INFO     	| Interval 1 (embedding) time 9.99426 s
2023-12-19 23:31:42 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:31:42 INFO     initial_embed: (1518, 128)
2023-12-19 23:31:42 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.5770567655563354, MSE Loss: 0.22258253395557404, ACC loss: 0.9315310120582581
Epoch 200, Loss: 0.4889652132987976, MSE Loss: 0.19150130450725555, ACC loss: 0.7864291071891785
2023-12-19 23:31:50 INFO     		| Interval 1 (training the model) time 8.07804 s
2023-12-19 23:31:50 INFO     			Refinement at level 3 completed.
2023-12-19 23:31:50 INFO     			Refinement at level 2 completed.
2023-12-19 23:31:50 INFO     			Refinement at level 1 completed.
2023-12-19 23:31:50 INFO     		| Interval 2 (refinement) time 0.04881 s
2023-12-19 23:31:50 INFO     	| Interval 2 (refinement training and applying) time 8.12833 s
2023-12-19 23:31:50 INFO     | Time for this section (main program): 18.36541 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.227445125579834
The empirical coverage with aps is: 0.9128127098083496
The empirical efficiency with daps is: 5.088703632354736
The empirical coverage with daps is: 0.9014405012130737
2023-12-19 23:31:53.261224: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:32:01 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.5, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:32:02 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:32:02 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:32:02 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:32:02 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:32:02 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:32:02 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:32:02 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:32:02 INFO     	| Interval 0 (graph coarsening) time 0.24094 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:32:02 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:32:13 INFO     	| Interval 1 (embedding) time 11.16493 s
2023-12-19 23:32:13 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:32:13 INFO     initial_embed: (1518, 128)
2023-12-19 23:32:13 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6568953394889832, MSE Loss: 0.22915613651275635, ACC loss: 1.08463454246521
Epoch 200, Loss: 0.5485755205154419, MSE Loss: 0.18866486847400665, ACC loss: 0.9084861278533936
2023-12-19 23:32:22 INFO     		| Interval 1 (training the model) time 9.33991 s
2023-12-19 23:32:22 INFO     			Refinement at level 3 completed.
2023-12-19 23:32:22 INFO     			Refinement at level 2 completed.
2023-12-19 23:32:22 INFO     			Refinement at level 1 completed.
2023-12-19 23:32:22 INFO     		| Interval 2 (refinement) time 0.06286 s
2023-12-19 23:32:22 INFO     	| Interval 2 (refinement training and applying) time 9.40435 s
2023-12-19 23:32:22 INFO     | Time for this section (main program): 20.81022 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.96512508392334
The empirical coverage with aps is: 0.8923426866531372
The empirical efficiency with daps is: 4.997725486755371
The empirical coverage with daps is: 0.8946171402931213
2023-12-19 23:32:25.996248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:32:34 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.5, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:32:34 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:32:34 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:32:34 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:32:34 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:32:34 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:32:34 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:32:34 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:32:34 INFO     	| Interval 0 (graph coarsening) time 0.24140 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:32:34 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:32:44 INFO     	| Interval 1 (embedding) time 9.43133 s
2023-12-19 23:32:44 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:32:44 INFO     initial_embed: (1518, 128)
2023-12-19 23:32:44 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.9544622898101807, MSE Loss: 0.3807503283023834, ACC loss: 1.5281742811203003
Epoch 200, Loss: 0.7674241065979004, MSE Loss: 0.24798032641410828, ACC loss: 1.2868678569793701
2023-12-19 23:32:51 INFO     		| Interval 1 (training the model) time 7.61713 s
2023-12-19 23:32:51 INFO     			Refinement at level 3 completed.
2023-12-19 23:32:52 INFO     			Refinement at level 2 completed.
2023-12-19 23:32:52 INFO     			Refinement at level 1 completed.
2023-12-19 23:32:52 INFO     		| Interval 2 (refinement) time 0.05322 s
2023-12-19 23:32:52 INFO     	| Interval 2 (refinement training and applying) time 7.67179 s
2023-12-19 23:32:52 INFO     | Time for this section (main program): 17.34452 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.995450973510742
The empirical coverage with aps is: 0.8984078764915466
The empirical efficiency with daps is: 4.959817886352539
The empirical coverage with daps is: 0.8984078764915466
2023-12-19 23:32:56.818180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:33:05 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.7, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:33:05 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:33:05 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:33:05 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:33:05 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:33:05 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:33:05 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:33:05 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:33:05 INFO     	| Interval 0 (graph coarsening) time 0.24515 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:33:05 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:33:16 INFO     	| Interval 1 (embedding) time 10.87704 s
2023-12-19 23:33:16 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:33:16 INFO     initial_embed: (1518, 128)
2023-12-19 23:33:16 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.619128942489624, MSE Loss: 0.4264124035835266, ACC loss: 0.7017217874526978
Epoch 200, Loss: 0.42725300788879395, MSE Loss: 0.39592358469963074, ACC loss: 0.4406799077987671
2023-12-19 23:33:24 INFO     		| Interval 1 (training the model) time 8.20946 s
2023-12-19 23:33:24 INFO     			Refinement at level 3 completed.
2023-12-19 23:33:24 INFO     			Refinement at level 2 completed.
2023-12-19 23:33:24 INFO     			Refinement at level 1 completed.
2023-12-19 23:33:24 INFO     		| Interval 2 (refinement) time 0.05010 s
2023-12-19 23:33:24 INFO     	| Interval 2 (refinement training and applying) time 8.26098 s
2023-12-19 23:33:24 INFO     | Time for this section (main program): 19.38318 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.012888431549072
The empirical coverage with aps is: 0.8847611546516418
The empirical efficiency with daps is: 5.087945461273193
The empirical coverage with daps is: 0.8953753113746643
2023-12-19 23:33:29.775258: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:33:37 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.7, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:33:38 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:33:38 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:33:38 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:33:38 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:33:38 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:33:38 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:33:38 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:33:38 INFO     	| Interval 0 (graph coarsening) time 0.24314 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:33:38 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:33:49 INFO     	| Interval 1 (embedding) time 11.08748 s
2023-12-19 23:33:49 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:33:49 INFO     initial_embed: (1518, 128)
2023-12-19 23:33:49 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6902135014533997, MSE Loss: 0.3652862310409546, ACC loss: 0.8294680714607239
Epoch 200, Loss: 0.6004868745803833, MSE Loss: 0.3026386797428131, ACC loss: 0.7281361818313599
2023-12-19 23:33:56 INFO     		| Interval 1 (training the model) time 7.42057 s
2023-12-19 23:33:56 INFO     			Refinement at level 3 completed.
2023-12-19 23:33:56 INFO     			Refinement at level 2 completed.
2023-12-19 23:33:56 INFO     			Refinement at level 1 completed.
2023-12-19 23:33:56 INFO     		| Interval 2 (refinement) time 0.05179 s
2023-12-19 23:33:56 INFO     	| Interval 2 (refinement training and applying) time 7.47369 s
2023-12-19 23:33:56 INFO     | Time for this section (main program): 18.80432 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.939348220825195
The empirical coverage with aps is: 0.887793779373169
The empirical efficiency with daps is: 4.948445796966553
The empirical coverage with daps is: 0.8885519504547119
2023-12-19 23:34:00.045351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:34:08 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.7, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:34:09 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:34:09 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:34:09 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:34:09 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:34:09 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:34:09 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:34:09 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:34:09 INFO     	| Interval 0 (graph coarsening) time 0.24300 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:34:09 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:34:20 INFO     	| Interval 1 (embedding) time 10.96187 s
2023-12-19 23:34:20 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:34:20 INFO     initial_embed: (1518, 128)
2023-12-19 23:34:20 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7733464241027832, MSE Loss: 0.408161461353302, ACC loss: 0.9298542737960815
Epoch 200, Loss: 0.6625558733940125, MSE Loss: 0.3436788022518158, ACC loss: 0.7992174625396729
2023-12-19 23:34:28 INFO     		| Interval 1 (training the model) time 8.21409 s
2023-12-19 23:34:28 INFO     			Refinement at level 3 completed.
2023-12-19 23:34:28 INFO     			Refinement at level 2 completed.
2023-12-19 23:34:28 INFO     			Refinement at level 1 completed.
2023-12-19 23:34:28 INFO     		| Interval 2 (refinement) time 0.05401 s
2023-12-19 23:34:28 INFO     	| Interval 2 (refinement training and applying) time 8.26953 s
2023-12-19 23:34:28 INFO     | Time for this section (main program): 19.47440 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.843821048736572
The empirical coverage with aps is: 0.8824867606163025
The empirical efficiency with daps is: 4.874905109405518
The empirical coverage with daps is: 0.8794541358947754
2023-12-19 23:34:31.003483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:34:39 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.7, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:34:40 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:34:40 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:34:40 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:34:40 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:34:40 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:34:40 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:34:40 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:34:40 INFO     	| Interval 0 (graph coarsening) time 0.24265 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:34:40 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:34:50 INFO     	| Interval 1 (embedding) time 10.18612 s
2023-12-19 23:34:50 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:34:50 INFO     initial_embed: (1518, 128)
2023-12-19 23:34:50 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.1006542444229126, MSE Loss: 0.5204909443855286, ACC loss: 1.3492956161499023
Epoch 200, Loss: 0.8871123790740967, MSE Loss: 0.4860761761665344, ACC loss: 1.0589849948883057
2023-12-19 23:34:58 INFO     		| Interval 1 (training the model) time 8.23843 s
2023-12-19 23:34:58 INFO     			Refinement at level 3 completed.
2023-12-19 23:34:58 INFO     			Refinement at level 2 completed.
2023-12-19 23:34:58 INFO     			Refinement at level 1 completed.
2023-12-19 23:34:58 INFO     		| Interval 2 (refinement) time 0.05115 s
2023-12-19 23:34:58 INFO     	| Interval 2 (refinement training and applying) time 8.29181 s
2023-12-19 23:34:58 INFO     | Time for this section (main program): 18.72057 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.019711971282959
The empirical coverage with aps is: 0.8847611546516418
The empirical efficiency with daps is: 5.029567718505859
The empirical coverage with daps is: 0.887035608291626
2023-12-19 23:35:01.445608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:35:10 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.9, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:35:10 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:35:10 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:35:10 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:35:10 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:35:10 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:35:10 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:35:10 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:35:10 INFO     	| Interval 0 (graph coarsening) time 0.24345 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:35:10 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:35:21 INFO     	| Interval 1 (embedding) time 11.31923 s
2023-12-19 23:35:21 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:35:21 INFO     initial_embed: (1518, 128)
2023-12-19 23:35:21 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6421443223953247, MSE Loss: 0.8567438125610352, ACC loss: 0.6182999014854431
Epoch 200, Loss: 0.33406755328178406, MSE Loss: 0.8317357897758484, ACC loss: 0.2787711024284363
2023-12-19 23:35:30 INFO     		| Interval 1 (training the model) time 8.42230 s
2023-12-19 23:35:30 INFO     			Refinement at level 3 completed.
2023-12-19 23:35:30 INFO     			Refinement at level 2 completed.
2023-12-19 23:35:30 INFO     			Refinement at level 1 completed.
2023-12-19 23:35:30 INFO     		| Interval 2 (refinement) time 0.06838 s
2023-12-19 23:35:30 INFO     	| Interval 2 (refinement training and applying) time 8.49283 s
2023-12-19 23:35:30 INFO     | Time for this section (main program): 20.05551 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.170583724975586
The empirical coverage with aps is: 0.887793779373169
The empirical efficiency with daps is: 5.187263011932373
The empirical coverage with daps is: 0.8809704184532166
2023-12-19 23:35:34.444264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:35:43 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.9, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:35:43 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:35:43 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:35:43 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:35:43 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:35:43 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:35:43 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:35:43 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:35:43 INFO     	| Interval 0 (graph coarsening) time 0.24428 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:35:43 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:35:53 INFO     	| Interval 1 (embedding) time 10.44118 s
2023-12-19 23:35:53 INFO     		| Interval 0 (double-base embedding) time 0.00003 s
2023-12-19 23:35:53 INFO     initial_embed: (1518, 128)
2023-12-19 23:35:53 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7446697354316711, MSE Loss: 0.807397186756134, ACC loss: 0.7376999855041504
Epoch 200, Loss: 0.6431273221969604, MSE Loss: 0.7966039180755615, ACC loss: 0.6260743737220764
2023-12-19 23:36:03 INFO     		| Interval 1 (training the model) time 9.25541 s
2023-12-19 23:36:03 INFO     			Refinement at level 3 completed.
2023-12-19 23:36:03 INFO     			Refinement at level 2 completed.
2023-12-19 23:36:03 INFO     			Refinement at level 1 completed.
2023-12-19 23:36:03 INFO     		| Interval 2 (refinement) time 0.06334 s
2023-12-19 23:36:03 INFO     	| Interval 2 (refinement training and applying) time 9.32125 s
2023-12-19 23:36:03 INFO     | Time for this section (main program): 20.00670 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.137983322143555
The empirical coverage with aps is: 0.8984078764915466
The empirical efficiency with daps is: 5.095526695251465
The empirical coverage with daps is: 0.8999241590499878
2023-12-19 23:36:07.120267: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:36:15 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.9, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:36:15 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:36:15 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:36:15 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:36:15 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:36:15 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:36:15 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:36:15 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:36:15 INFO     	| Interval 0 (graph coarsening) time 0.24621 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:36:15 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:36:29 INFO     	| Interval 1 (embedding) time 13.49410 s
2023-12-19 23:36:29 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:36:29 INFO     initial_embed: (1518, 128)
2023-12-19 23:36:29 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.8048834204673767, MSE Loss: 0.9609545469284058, ACC loss: 0.7875422239303589
Epoch 200, Loss: 0.7125113606452942, MSE Loss: 0.8586772084236145, ACC loss: 0.696270763874054
2023-12-19 23:36:37 INFO     		| Interval 1 (training the model) time 8.04703 s
2023-12-19 23:36:37 INFO     			Refinement at level 3 completed.
2023-12-19 23:36:37 INFO     			Refinement at level 2 completed.
2023-12-19 23:36:37 INFO     			Refinement at level 1 completed.
2023-12-19 23:36:37 INFO     		| Interval 2 (refinement) time 0.05119 s
2023-12-19 23:36:37 INFO     	| Interval 2 (refinement training and applying) time 8.09972 s
2023-12-19 23:36:37 INFO     | Time for this section (main program): 21.84003 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.093252658843994
The empirical coverage with aps is: 0.8938589692115784
The empirical efficiency with daps is: 5.107657432556152
The empirical coverage with daps is: 0.9021986126899719
2023-12-19 23:36:40.223134: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:36:48 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.9, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:36:48 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:36:49 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:36:49 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:36:49 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:36:49 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:36:49 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:36:49 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:36:49 INFO     	| Interval 0 (graph coarsening) time 0.24332 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:36:49 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:36:59 INFO     	| Interval 1 (embedding) time 10.43927 s
2023-12-19 23:36:59 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:36:59 INFO     initial_embed: (1518, 128)
2023-12-19 23:36:59 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.1362600326538086, MSE Loss: 1.029098629951477, ACC loss: 1.1481670141220093
Epoch 200, Loss: 0.9171801805496216, MSE Loss: 1.0835604667663574, ACC loss: 0.8986935019493103
2023-12-19 23:37:07 INFO     		| Interval 1 (training the model) time 8.02447 s
2023-12-19 23:37:07 INFO     			Refinement at level 3 completed.
2023-12-19 23:37:07 INFO     			Refinement at level 2 completed.
2023-12-19 23:37:07 INFO     			Refinement at level 1 completed.
2023-12-19 23:37:07 INFO     		| Interval 2 (refinement) time 0.05135 s
2023-12-19 23:37:07 INFO     	| Interval 2 (refinement training and applying) time 8.07781 s
2023-12-19 23:37:07 INFO     | Time for this section (main program): 18.76039 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.259287357330322
The empirical coverage with aps is: 0.9143290519714355
The empirical efficiency with daps is: 5.226686954498291
The empirical coverage with daps is: 0.9173616170883179
2023-12-19 23:37:10.337421: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:37:18 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.99, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:37:18 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:37:19 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:37:19 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:37:19 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:37:19 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:37:19 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:37:19 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:37:19 INFO     	| Interval 0 (graph coarsening) time 0.24325 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:37:19 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:37:31 INFO     	| Interval 1 (embedding) time 12.22480 s
2023-12-19 23:37:31 INFO     		| Interval 0 (double-base embedding) time 0.00003 s
2023-12-19 23:37:31 INFO     initial_embed: (1518, 128)
2023-12-19 23:37:31 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6727265119552612, MSE Loss: 1.4043259620666504, ACC loss: 0.6653366088867188
Epoch 200, Loss: 0.2991587519645691, MSE Loss: 1.5050536394119263, ACC loss: 0.28697800636291504
2023-12-19 23:37:40 INFO     		| Interval 1 (training the model) time 9.45162 s
2023-12-19 23:37:40 INFO     			Refinement at level 3 completed.
2023-12-19 23:37:40 INFO     			Refinement at level 2 completed.
2023-12-19 23:37:40 INFO     			Refinement at level 1 completed.
2023-12-19 23:37:40 INFO     		| Interval 2 (refinement) time 0.06246 s
2023-12-19 23:37:40 INFO     	| Interval 2 (refinement training and applying) time 9.51668 s
2023-12-19 23:37:40 INFO     | Time for this section (main program): 21.98472 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.295678615570068
The empirical coverage with aps is: 0.8794541358947754
The empirical efficiency with daps is: 5.266110897064209
The empirical coverage with daps is: 0.872630774974823
2023-12-19 23:37:44.230024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:37:52 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.99, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:37:53 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:37:53 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:37:53 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:37:53 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:37:53 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:37:53 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:37:53 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:37:53 INFO     	| Interval 0 (graph coarsening) time 0.24655 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:37:53 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:38:05 INFO     	| Interval 1 (embedding) time 12.32028 s
2023-12-19 23:38:05 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:38:05 INFO     initial_embed: (1518, 128)
2023-12-19 23:38:05 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.8063977956771851, MSE Loss: 1.4313629865646362, ACC loss: 0.8000850081443787
Epoch 200, Loss: 0.6425024271011353, MSE Loss: 1.5407428741455078, ACC loss: 0.6334292888641357
2023-12-19 23:38:14 INFO     		| Interval 1 (training the model) time 9.26150 s
2023-12-19 23:38:14 INFO     			Refinement at level 3 completed.
2023-12-19 23:38:14 INFO     			Refinement at level 2 completed.
2023-12-19 23:38:14 INFO     			Refinement at level 1 completed.
2023-12-19 23:38:14 INFO     		| Interval 2 (refinement) time 0.05419 s
2023-12-19 23:38:14 INFO     	| Interval 2 (refinement training and applying) time 9.31727 s
2023-12-19 23:38:14 INFO     | Time for this section (main program): 21.88409 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.301743507385254
The empirical coverage with aps is: 0.905989408493042
The empirical efficiency with daps is: 5.288855075836182
The empirical coverage with daps is: 0.9014405012130737
2023-12-19 23:38:17.632820: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:38:26 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.99, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:38:26 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:38:26 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:38:26 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:38:27 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:38:27 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:38:27 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:38:27 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:38:27 INFO     	| Interval 0 (graph coarsening) time 0.24730 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:38:27 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:38:38 INFO     	| Interval 1 (embedding) time 11.32363 s
2023-12-19 23:38:38 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:38:38 INFO     initial_embed: (1518, 128)
2023-12-19 23:38:38 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.8519280552864075, MSE Loss: 1.5411934852600098, ACC loss: 0.8449658155441284
Epoch 200, Loss: 0.7346639633178711, MSE Loss: 1.4670650959014893, ACC loss: 0.7272659540176392
2023-12-19 23:38:46 INFO     		| Interval 1 (training the model) time 8.41630 s
2023-12-19 23:38:46 INFO     			Refinement at level 3 completed.
2023-12-19 23:38:46 INFO     			Refinement at level 2 completed.
2023-12-19 23:38:46 INFO     			Refinement at level 1 completed.
2023-12-19 23:38:46 INFO     		| Interval 2 (refinement) time 0.05022 s
2023-12-19 23:38:46 INFO     	| Interval 2 (refinement training and applying) time 8.46834 s
2023-12-19 23:38:46 INFO     | Time for this section (main program): 20.03926 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.3699774742126465
The empirical coverage with aps is: 0.8931008577346802
The empirical efficiency with daps is: 5.443517684936523
The empirical coverage with daps is: 0.9075056910514832
2023-12-19 23:38:49.727457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:38:57 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=200, jobid=92499, lambda_fl=0.99, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:38:58 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:38:58 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:38:58 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:38:58 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:38:58 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:38:58 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:38:58 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:38:58 INFO     	| Interval 0 (graph coarsening) time 0.24490 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:38:58 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:39:09 INFO     	| Interval 1 (embedding) time 11.45411 s
2023-12-19 23:39:09 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:39:09 INFO     initial_embed: (1518, 128)
2023-12-19 23:39:09 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.0939233303070068, MSE Loss: 1.7617943286895752, ACC loss: 1.0871771574020386
Epoch 200, Loss: 0.874697744846344, MSE Loss: 1.7942240238189697, ACC loss: 0.8654096126556396
2023-12-19 23:39:17 INFO     		| Interval 1 (training the model) time 8.27055 s
2023-12-19 23:39:17 INFO     			Refinement at level 3 completed.
2023-12-19 23:39:17 INFO     			Refinement at level 2 completed.
2023-12-19 23:39:17 INFO     			Refinement at level 1 completed.
2023-12-19 23:39:17 INFO     		| Interval 2 (refinement) time 0.05047 s
2023-12-19 23:39:17 INFO     	| Interval 2 (refinement training and applying) time 8.32263 s
2023-12-19 23:39:17 INFO     | Time for this section (main program): 20.02163 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.589082717895508
The empirical coverage with aps is: 0.9150871634483337
The empirical efficiency with daps is: 5.583017349243164
The empirical coverage with daps is: 0.905989408493042
2023-12-19 23:39:20.987682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:39:29 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.1, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:39:29 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:39:29 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:39:29 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:39:29 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:39:29 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:39:29 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:39:29 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:39:29 INFO     	| Interval 0 (graph coarsening) time 0.24407 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:39:29 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:39:41 INFO     	| Interval 1 (embedding) time 11.25261 s
2023-12-19 23:39:41 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:39:41 INFO     initial_embed: (1518, 128)
2023-12-19 23:39:41 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.2568800747394562, MSE Loss: 0.15093713998794556, ACC loss: 1.2103663682937622
Epoch 200, Loss: 0.17269855737686157, MSE Loss: 0.0831591859459877, ACC loss: 0.9785528182983398
Epoch 300, Loss: 0.13896414637565613, MSE Loss: 0.0587581992149353, ACC loss: 0.8608177304267883
Epoch 400, Loss: 0.11797906458377838, MSE Loss: 0.048072583973407745, ACC loss: 0.7471374273300171
Epoch 500, Loss: 0.11066975444555283, MSE Loss: 0.04578808322548866, ACC loss: 0.6946048140525818
2023-12-19 23:40:01 INFO     		| Interval 1 (training the model) time 20.39840 s
2023-12-19 23:40:01 INFO     			Refinement at level 3 completed.
2023-12-19 23:40:01 INFO     			Refinement at level 2 completed.
2023-12-19 23:40:01 INFO     			Refinement at level 1 completed.
2023-12-19 23:40:01 INFO     		| Interval 2 (refinement) time 0.05395 s
2023-12-19 23:40:01 INFO     	| Interval 2 (refinement training and applying) time 20.45392 s
2023-12-19 23:40:01 INFO     | Time for this section (main program): 31.95060 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.231994152069092
The empirical coverage with aps is: 0.8984078764915466
The empirical efficiency with daps is: 5.208491325378418
The empirical coverage with daps is: 0.8961334228515625
2023-12-19 23:40:04.448363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:40:13 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.1, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:40:13 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:40:13 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:40:13 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:40:13 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:40:13 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:40:13 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:40:13 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:40:13 INFO     	| Interval 0 (graph coarsening) time 0.24354 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:40:13 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:40:23 INFO     	| Interval 1 (embedding) time 9.88668 s
2023-12-19 23:40:23 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:40:23 INFO     initial_embed: (1518, 128)
2023-12-19 23:40:23 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.22957323491573334, MSE Loss: 0.11345512419939041, ACC loss: 1.2746362686157227
Epoch 200, Loss: 0.17020949721336365, MSE Loss: 0.06723849475383759, ACC loss: 1.0969486236572266
Epoch 300, Loss: 0.13492989540100098, MSE Loss: 0.04578874260187149, ACC loss: 0.9372002482414246
Epoch 400, Loss: 0.1287681758403778, MSE Loss: 0.04277794435620308, ACC loss: 0.9026802182197571
Epoch 500, Loss: 0.11789590120315552, MSE Loss: 0.037923164665699005, ACC loss: 0.8376504778862
2023-12-19 23:40:44 INFO     		| Interval 1 (training the model) time 20.86552 s
2023-12-19 23:40:44 INFO     			Refinement at level 3 completed.
2023-12-19 23:40:44 INFO     			Refinement at level 2 completed.
2023-12-19 23:40:44 INFO     			Refinement at level 1 completed.
2023-12-19 23:40:44 INFO     		| Interval 2 (refinement) time 0.05180 s
2023-12-19 23:40:44 INFO     	| Interval 2 (refinement training and applying) time 20.91919 s
2023-12-19 23:40:44 INFO     | Time for this section (main program): 31.04940 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.175890922546387
The empirical coverage with aps is: 0.9150871634483337
The empirical efficiency with daps is: 5.11675500869751
The empirical coverage with daps is: 0.9067475199699402
2023-12-19 23:40:46.874422: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:40:55 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.1, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:40:55 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:40:55 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:40:55 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:40:55 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:40:55 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:40:55 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:40:55 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:40:55 INFO     	| Interval 0 (graph coarsening) time 0.24051 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:40:55 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:41:05 INFO     	| Interval 1 (embedding) time 9.98668 s
2023-12-19 23:41:05 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:41:05 INFO     initial_embed: (1518, 128)
2023-12-19 23:41:05 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.24187442660331726, MSE Loss: 0.11119329929351807, ACC loss: 1.4180046319961548
Epoch 200, Loss: 0.17006997764110565, MSE Loss: 0.056540559977293015, ACC loss: 1.1918346881866455
Epoch 300, Loss: 0.14608892798423767, MSE Loss: 0.04377259686589241, ACC loss: 1.0669358968734741
Epoch 400, Loss: 0.13475586473941803, MSE Loss: 0.039386410266160965, ACC loss: 0.9930809140205383
Epoch 500, Loss: 0.12819123268127441, MSE Loss: 0.037097178399562836, ACC loss: 0.9480377435684204
2023-12-19 23:41:27 INFO     		| Interval 1 (training the model) time 21.50798 s
2023-12-19 23:41:27 INFO     			Refinement at level 3 completed.
2023-12-19 23:41:27 INFO     			Refinement at level 2 completed.
2023-12-19 23:41:27 INFO     			Refinement at level 1 completed.
2023-12-19 23:41:27 INFO     		| Interval 2 (refinement) time 0.06368 s
2023-12-19 23:41:27 INFO     	| Interval 2 (refinement training and applying) time 21.57373 s
2023-12-19 23:41:27 INFO     | Time for this section (main program): 31.80091 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.109931945800781
The empirical coverage with aps is: 0.8946171402931213
The empirical efficiency with daps is: 5.1432905197143555
The empirical coverage with daps is: 0.9006823301315308
2023-12-19 23:41:30.565352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:41:39 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.1, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:41:39 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:41:39 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:41:39 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:41:39 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:41:39 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:41:39 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:41:39 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:41:39 INFO     	| Interval 0 (graph coarsening) time 0.24329 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:41:39 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:41:51 INFO     	| Interval 1 (embedding) time 11.42018 s
2023-12-19 23:41:51 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:41:51 INFO     initial_embed: (1518, 128)
2023-12-19 23:41:51 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.43507346510887146, MSE Loss: 0.2989768385887146, ACC loss: 1.6599431037902832
Epoch 200, Loss: 0.28038620948791504, MSE Loss: 0.13897359371185303, ACC loss: 1.5530996322631836
Epoch 300, Loss: 0.2241349220275879, MSE Loss: 0.0863613411784172, ACC loss: 1.4640971422195435
Epoch 400, Loss: 0.19566012918949127, MSE Loss: 0.06335368007421494, ACC loss: 1.3864182233810425
Epoch 500, Loss: 0.17856143414974213, MSE Loss: 0.051844026893377304, ACC loss: 1.3190181255340576
2023-12-19 23:42:11 INFO     		| Interval 1 (training the model) time 20.29672 s
2023-12-19 23:42:11 INFO     			Refinement at level 3 completed.
2023-12-19 23:42:11 INFO     			Refinement at level 2 completed.
2023-12-19 23:42:11 INFO     			Refinement at level 1 completed.
2023-12-19 23:42:11 INFO     		| Interval 2 (refinement) time 0.06158 s
2023-12-19 23:42:11 INFO     	| Interval 2 (refinement training and applying) time 20.35953 s
2023-12-19 23:42:11 INFO     | Time for this section (main program): 32.02300 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.881728649139404
The empirical coverage with aps is: 0.9090219736099243
The empirical efficiency with daps is: 4.890826225280762
The empirical coverage with daps is: 0.9105383157730103
2023-12-19 23:42:14.695426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:42:23 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.3, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:42:23 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:42:23 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:42:23 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:42:23 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:42:23 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:42:23 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:42:23 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:42:23 INFO     	| Interval 0 (graph coarsening) time 0.24257 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:42:23 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:42:33 INFO     	| Interval 1 (embedding) time 9.98563 s
2023-12-19 23:42:33 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:42:33 INFO     initial_embed: (1518, 128)
2023-12-19 23:42:33 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7165247201919556, MSE Loss: 0.4273182153701782, ACC loss: 1.391339898109436
Epoch 200, Loss: 0.3287898600101471, MSE Loss: 0.1523353010416031, ACC loss: 0.7405171394348145
Epoch 300, Loss: 0.24674800038337708, MSE Loss: 0.12740439176559448, ACC loss: 0.5252164006233215
Epoch 400, Loss: 0.2638772428035736, MSE Loss: 0.13006436824798584, ACC loss: 0.5761072635650635
Epoch 500, Loss: 0.17647910118103027, MSE Loss: 0.1114446297287941, ACC loss: 0.3282262086868286
2023-12-19 23:42:55 INFO     		| Interval 1 (training the model) time 21.23310 s
2023-12-19 23:42:55 INFO     			Refinement at level 3 completed.
2023-12-19 23:42:55 INFO     			Refinement at level 2 completed.
2023-12-19 23:42:55 INFO     			Refinement at level 1 completed.
2023-12-19 23:42:55 INFO     		| Interval 2 (refinement) time 0.05859 s
2023-12-19 23:42:55 INFO     	| Interval 2 (refinement training and applying) time 21.29370 s
2023-12-19 23:42:55 INFO     | Time for this section (main program): 31.52190 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.2160725593566895
The empirical coverage with aps is: 0.9082638621330261
The empirical efficiency with daps is: 5.140257835388184
The empirical coverage with daps is: 0.9014405012130737
2023-12-19 23:42:58.355181: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:43:07 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.3, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:43:07 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:43:07 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:43:07 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:43:07 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:43:07 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:43:07 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:43:07 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:43:07 INFO     	| Interval 0 (graph coarsening) time 0.24311 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:43:07 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:43:19 INFO     	| Interval 1 (embedding) time 12.32146 s
2023-12-19 23:43:19 INFO     		| Interval 0 (double-base embedding) time 0.00003 s
2023-12-19 23:43:19 INFO     initial_embed: (1518, 128)
2023-12-19 23:43:19 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.41338804364204407, MSE Loss: 0.137653648853302, ACC loss: 1.0567682981491089
Epoch 200, Loss: 0.3395669162273407, MSE Loss: 0.1083064004778862, ACC loss: 0.8791747093200684
Epoch 300, Loss: 0.3099556863307953, MSE Loss: 0.08823294192552567, ACC loss: 0.8273087739944458
Epoch 400, Loss: 0.29605627059936523, MSE Loss: 0.0856759324669838, ACC loss: 0.7869436740875244
Epoch 500, Loss: 0.24996697902679443, MSE Loss: 0.08041493594646454, ACC loss: 0.6455883383750916
2023-12-19 23:43:43 INFO     		| Interval 1 (training the model) time 23.34603 s
2023-12-19 23:43:43 INFO     			Refinement at level 3 completed.
2023-12-19 23:43:43 INFO     			Refinement at level 2 completed.
2023-12-19 23:43:43 INFO     			Refinement at level 1 completed.
2023-12-19 23:43:43 INFO     		| Interval 2 (refinement) time 0.06181 s
2023-12-19 23:43:43 INFO     	| Interval 2 (refinement training and applying) time 23.41050 s
2023-12-19 23:43:43 INFO     | Time for this section (main program): 35.97506 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.049279689788818
The empirical coverage with aps is: 0.8984078764915466
The empirical efficiency with daps is: 4.981046199798584
The empirical coverage with daps is: 0.8931008577346802
2023-12-19 23:43:46.215900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:43:54 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.3, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:43:54 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:43:54 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:43:54 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:43:54 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:43:54 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:43:54 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:43:54 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:43:54 INFO     	| Interval 0 (graph coarsening) time 0.24525 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:43:54 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:44:06 INFO     	| Interval 1 (embedding) time 11.73303 s
2023-12-19 23:44:06 INFO     		| Interval 0 (double-base embedding) time 0.00004 s
2023-12-19 23:44:06 INFO     initial_embed: (1518, 128)
2023-12-19 23:44:06 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.47247961163520813, MSE Loss: 0.14836615324020386, ACC loss: 1.228744387626648
Epoch 200, Loss: 0.37093785405158997, MSE Loss: 0.10236717760562897, ACC loss: 0.9976027011871338
Epoch 300, Loss: 0.33700644969940186, MSE Loss: 0.0918794795870781, ACC loss: 0.9089693427085876
Epoch 400, Loss: 0.31754007935523987, MSE Loss: 0.08248648047447205, ACC loss: 0.865998387336731
Epoch 500, Loss: 0.299365371465683, MSE Loss: 0.08004751056432724, ACC loss: 0.8111069798469543
2023-12-19 23:44:28 INFO     		| Interval 1 (training the model) time 21.57852 s
2023-12-19 23:44:28 INFO     			Refinement at level 3 completed.
2023-12-19 23:44:28 INFO     			Refinement at level 2 completed.
2023-12-19 23:44:28 INFO     			Refinement at level 1 completed.
2023-12-19 23:44:28 INFO     		| Interval 2 (refinement) time 0.06047 s
2023-12-19 23:44:28 INFO     	| Interval 2 (refinement training and applying) time 21.64141 s
2023-12-19 23:44:28 INFO     | Time for this section (main program): 33.61969 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.1607279777526855
The empirical coverage with aps is: 0.905989408493042
The empirical efficiency with daps is: 5.125853061676025
The empirical coverage with daps is: 0.9021986126899719
2023-12-19 23:44:30.998495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:44:39 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.3, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:44:39 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:44:39 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:44:40 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:44:40 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:44:40 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:44:40 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:44:40 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:44:40 INFO     	| Interval 0 (graph coarsening) time 0.24358 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:44:40 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:44:51 INFO     	| Interval 1 (embedding) time 11.90574 s
2023-12-19 23:44:51 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:44:52 INFO     initial_embed: (1518, 128)
2023-12-19 23:44:52 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7070063352584839, MSE Loss: 0.3233255445957184, ACC loss: 1.6022614240646362
Epoch 200, Loss: 0.5431872010231018, MSE Loss: 0.16274939477443695, ACC loss: 1.4308754205703735
Epoch 300, Loss: 0.4684711992740631, MSE Loss: 0.12246353924274445, ACC loss: 1.275822401046753
Epoch 400, Loss: 0.4236954152584076, MSE Loss: 0.10780274122953415, ACC loss: 1.160778284072876
Epoch 500, Loss: 0.39431077241897583, MSE Loss: 0.09942896664142609, ACC loss: 1.08236825466156
2023-12-19 23:45:12 INFO     		| Interval 1 (training the model) time 20.42599 s
2023-12-19 23:45:12 INFO     			Refinement at level 3 completed.
2023-12-19 23:45:12 INFO     			Refinement at level 2 completed.
2023-12-19 23:45:12 INFO     			Refinement at level 1 completed.
2023-12-19 23:45:12 INFO     		| Interval 2 (refinement) time 0.05452 s
2023-12-19 23:45:12 INFO     	| Interval 2 (refinement training and applying) time 20.48253 s
2023-12-19 23:45:12 INFO     | Time for this section (main program): 32.63185 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.016679286956787
The empirical coverage with aps is: 0.8961334228515625
The empirical efficiency with daps is: 5.166034698486328
The empirical coverage with daps is: 0.9120545983314514
2023-12-19 23:45:15.095576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:45:23 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.5, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:45:24 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:45:24 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:45:24 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:45:24 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:45:24 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:45:24 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:45:24 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:45:24 INFO     	| Interval 0 (graph coarsening) time 0.24428 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:45:24 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:45:34 INFO     	| Interval 1 (embedding) time 10.24412 s
2023-12-19 23:45:34 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:45:34 INFO     initial_embed: (1518, 128)
2023-12-19 23:45:34 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6417176127433777, MSE Loss: 0.27336910367012024, ACC loss: 1.0100661516189575
Epoch 200, Loss: 0.41048872470855713, MSE Loss: 0.2410120666027069, ACC loss: 0.579965353012085
Epoch 300, Loss: 0.432142972946167, MSE Loss: 0.23160867393016815, ACC loss: 0.6326772570610046
Epoch 400, Loss: 0.26221373677253723, MSE Loss: 0.2218616008758545, ACC loss: 0.30256587266921997
Epoch 500, Loss: 0.21893221139907837, MSE Loss: 0.20455989241600037, ACC loss: 0.23330454528331757
2023-12-19 23:45:56 INFO     		| Interval 1 (training the model) time 21.65991 s
2023-12-19 23:45:56 INFO     			Refinement at level 3 completed.
2023-12-19 23:45:56 INFO     			Refinement at level 2 completed.
2023-12-19 23:45:56 INFO     			Refinement at level 1 completed.
2023-12-19 23:45:56 INFO     		| Interval 2 (refinement) time 0.05534 s
2023-12-19 23:45:56 INFO     	| Interval 2 (refinement training and applying) time 21.71692 s
2023-12-19 23:45:56 INFO     | Time for this section (main program): 32.20533 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.041698455810547
The empirical coverage with aps is: 0.9029567837715149
The empirical efficiency with daps is: 5.028051376342773
The empirical coverage with daps is: 0.9006823301315308
2023-12-19 23:45:59.060163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:46:07 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.5, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:46:08 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:46:08 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:46:08 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:46:08 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:46:08 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:46:08 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:46:08 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:46:08 INFO     	| Interval 0 (graph coarsening) time 0.24345 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:46:08 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:46:19 INFO     	| Interval 1 (embedding) time 11.11367 s
2023-12-19 23:46:19 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:46:19 INFO     initial_embed: (1518, 128)
2023-12-19 23:46:19 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.5762803554534912, MSE Loss: 0.2040778398513794, ACC loss: 0.9484828114509583
Epoch 200, Loss: 0.49695271253585815, MSE Loss: 0.17165176570415497, ACC loss: 0.8222536444664001
Epoch 300, Loss: 0.4357472360134125, MSE Loss: 0.159685418009758, ACC loss: 0.7118090391159058
Epoch 400, Loss: 0.35932332277297974, MSE Loss: 0.1566573679447174, ACC loss: 0.5619893074035645
Epoch 500, Loss: 0.3233882784843445, MSE Loss: 0.1573251485824585, ACC loss: 0.4894513785839081
2023-12-19 23:46:40 INFO     		| Interval 1 (training the model) time 21.66103 s
2023-12-19 23:46:40 INFO     			Refinement at level 3 completed.
2023-12-19 23:46:40 INFO     			Refinement at level 2 completed.
2023-12-19 23:46:41 INFO     			Refinement at level 1 completed.
2023-12-19 23:46:41 INFO     		| Interval 2 (refinement) time 0.05645 s
2023-12-19 23:46:41 INFO     	| Interval 2 (refinement training and applying) time 21.71918 s
2023-12-19 23:46:41 INFO     | Time for this section (main program): 33.07629 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.319939136505127
The empirical coverage with aps is: 0.9173616170883179
The empirical efficiency with daps is: 5.492039203643799
The empirical coverage with daps is: 0.9363154172897339
2023-12-19 23:46:44.338851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:46:53 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.5, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:46:53 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:46:53 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:46:53 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:46:53 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:46:53 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:46:53 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:46:53 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:46:53 INFO     	| Interval 0 (graph coarsening) time 0.24274 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:46:53 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:47:03 INFO     	| Interval 1 (embedding) time 9.77348 s
2023-12-19 23:47:03 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:47:03 INFO     initial_embed: (1518, 128)
2023-12-19 23:47:03 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6188092231750488, MSE Loss: 0.2588048577308655, ACC loss: 0.9788135290145874
Epoch 200, Loss: 0.5031852722167969, MSE Loss: 0.1940278857946396, ACC loss: 0.812342643737793
Epoch 300, Loss: 0.4544597864151001, MSE Loss: 0.17045554518699646, ACC loss: 0.7384640574455261
Epoch 400, Loss: 0.42097967863082886, MSE Loss: 0.164932519197464, ACC loss: 0.6770268082618713
Epoch 500, Loss: 0.38705986738204956, MSE Loss: 0.15087270736694336, ACC loss: 0.6232470273971558
2023-12-19 23:47:24 INFO     		| Interval 1 (training the model) time 21.38135 s
2023-12-19 23:47:24 INFO     			Refinement at level 3 completed.
2023-12-19 23:47:24 INFO     			Refinement at level 2 completed.
2023-12-19 23:47:24 INFO     			Refinement at level 1 completed.
2023-12-19 23:47:24 INFO     		| Interval 2 (refinement) time 0.05603 s
2023-12-19 23:47:24 INFO     	| Interval 2 (refinement training and applying) time 21.43863 s
2023-12-19 23:47:24 INFO     | Time for this section (main program): 31.45485 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.0644426345825195
The empirical coverage with aps is: 0.9014405012130737
The empirical efficiency with daps is: 5.019711971282959
The empirical coverage with daps is: 0.8961334228515625
2023-12-19 23:47:27.405324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:47:36 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.5, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:47:36 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:47:36 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:47:36 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:47:36 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:47:36 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:47:36 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:47:36 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:47:36 INFO     	| Interval 0 (graph coarsening) time 0.24214 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:47:36 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:47:49 INFO     	| Interval 1 (embedding) time 13.08093 s
2023-12-19 23:47:49 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:47:49 INFO     initial_embed: (1518, 128)
2023-12-19 23:47:49 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.9505265951156616, MSE Loss: 0.37528106570243835, ACC loss: 1.5257720947265625
Epoch 200, Loss: 0.7666471600532532, MSE Loss: 0.24980215728282928, ACC loss: 1.2834922075271606
Epoch 300, Loss: 0.6708154678344727, MSE Loss: 0.2209731638431549, ACC loss: 1.1206578016281128
Epoch 400, Loss: 0.6171592473983765, MSE Loss: 0.20393157005310059, ACC loss: 1.0303869247436523
Epoch 500, Loss: 0.5826478004455566, MSE Loss: 0.19077728688716888, ACC loss: 0.9745182991027832
2023-12-19 23:48:09 INFO     		| Interval 1 (training the model) time 20.29313 s
2023-12-19 23:48:09 INFO     			Refinement at level 3 completed.
2023-12-19 23:48:09 INFO     			Refinement at level 2 completed.
2023-12-19 23:48:09 INFO     			Refinement at level 1 completed.
2023-12-19 23:48:09 INFO     		| Interval 2 (refinement) time 0.05279 s
2023-12-19 23:48:09 INFO     	| Interval 2 (refinement training and applying) time 20.34784 s
2023-12-19 23:48:09 INFO     | Time for this section (main program): 33.67091 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.00758171081543
The empirical coverage with aps is: 0.8953753113746643
The empirical efficiency with daps is: 5.063684463500977
The empirical coverage with daps is: 0.904473066329956
2023-12-19 23:48:12.577078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:48:21 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.7, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:48:21 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:48:21 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:48:21 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:48:21 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:48:21 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:48:21 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:48:21 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:48:21 INFO     	| Interval 0 (graph coarsening) time 0.24378 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:48:21 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:48:34 INFO     	| Interval 1 (embedding) time 12.70559 s
2023-12-19 23:48:34 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:48:34 INFO     initial_embed: (1518, 128)
2023-12-19 23:48:34 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6250015497207642, MSE Loss: 0.4154454469680786, ACC loss: 0.714811384677887
Epoch 200, Loss: 0.3950764536857605, MSE Loss: 0.4178341329097748, ACC loss: 0.385323166847229
Epoch 300, Loss: 0.3205364942550659, MSE Loss: 0.3770768344402313, ACC loss: 0.29630494117736816
Epoch 400, Loss: 0.2331368625164032, MSE Loss: 0.3556326627731323, ACC loss: 0.1806386560201645
Epoch 500, Loss: 0.2018357813358307, MSE Loss: 0.3301282227039337, ACC loss: 0.14685329794883728
2023-12-19 23:48:55 INFO     		| Interval 1 (training the model) time 21.59156 s
2023-12-19 23:48:55 INFO     			Refinement at level 3 completed.
2023-12-19 23:48:55 INFO     			Refinement at level 2 completed.
2023-12-19 23:48:56 INFO     			Refinement at level 1 completed.
2023-12-19 23:48:56 INFO     		| Interval 2 (refinement) time 0.06261 s
2023-12-19 23:48:56 INFO     	| Interval 2 (refinement training and applying) time 21.65593 s
2023-12-19 23:48:56 INFO     | Time for this section (main program): 34.60530 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.147080898284912
The empirical coverage with aps is: 0.9090219736099243
The empirical efficiency with daps is: 5.227445125579834
The empirical coverage with daps is: 0.9120545983314514
2023-12-19 23:48:58.926180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:49:07 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.7, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:49:07 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:49:08 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:49:08 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:49:08 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:49:08 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:49:08 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:49:08 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:49:08 INFO     	| Interval 0 (graph coarsening) time 0.24378 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:49:08 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:49:19 INFO     	| Interval 1 (embedding) time 11.17160 s
2023-12-19 23:49:19 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:49:19 INFO     initial_embed: (1518, 128)
2023-12-19 23:49:19 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6795396208763123, MSE Loss: 0.37737923860549927, ACC loss: 0.8090369701385498
Epoch 200, Loss: 0.576990008354187, MSE Loss: 0.31041017174720764, ACC loss: 0.691238522529602
Epoch 300, Loss: 0.4834880232810974, MSE Loss: 0.3064400255680084, ACC loss: 0.5593657493591309
Epoch 400, Loss: 0.3949158787727356, MSE Loss: 0.27297452092170715, ACC loss: 0.447176456451416
Epoch 500, Loss: 0.3283911347389221, MSE Loss: 0.2546232342720032, ACC loss: 0.3600059151649475
2023-12-19 23:49:39 INFO     		| Interval 1 (training the model) time 20.50171 s
2023-12-19 23:49:39 INFO     			Refinement at level 3 completed.
2023-12-19 23:49:39 INFO     			Refinement at level 2 completed.
2023-12-19 23:49:39 INFO     			Refinement at level 1 completed.
2023-12-19 23:49:39 INFO     		| Interval 2 (refinement) time 0.05256 s
2023-12-19 23:49:39 INFO     	| Interval 2 (refinement training and applying) time 20.55554 s
2023-12-19 23:49:39 INFO     | Time for this section (main program): 31.97092 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.987111568450928
The empirical coverage with aps is: 0.8885519504547119
The empirical efficiency with daps is: 4.942380428314209
The empirical coverage with daps is: 0.8809704184532166
2023-12-19 23:49:42.480829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:49:51 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.7, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:49:51 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:49:51 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:49:51 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:49:51 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:49:51 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:49:51 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:49:51 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:49:51 INFO     	| Interval 0 (graph coarsening) time 0.24098 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:49:51 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:50:03 INFO     	| Interval 1 (embedding) time 12.03394 s
2023-12-19 23:50:03 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:50:03 INFO     initial_embed: (1518, 128)
2023-12-19 23:50:03 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7630558609962463, MSE Loss: 0.43139201402664185, ACC loss: 0.9051975607872009
Epoch 200, Loss: 0.6464499831199646, MSE Loss: 0.3653027415275574, ACC loss: 0.7669416666030884
Epoch 300, Loss: 0.5878015756607056, MSE Loss: 0.33012330532073975, ACC loss: 0.6982350945472717
Epoch 400, Loss: 0.5389742851257324, MSE Loss: 0.30834606289863586, ACC loss: 0.6378149390220642
Epoch 500, Loss: 0.45890647172927856, MSE Loss: 0.296590656042099, ACC loss: 0.5284703969955444
2023-12-19 23:50:23 INFO     		| Interval 1 (training the model) time 20.10354 s
2023-12-19 23:50:23 INFO     			Refinement at level 3 completed.
2023-12-19 23:50:23 INFO     			Refinement at level 2 completed.
2023-12-19 23:50:23 INFO     			Refinement at level 1 completed.
2023-12-19 23:50:23 INFO     		| Interval 2 (refinement) time 0.05039 s
2023-12-19 23:50:23 INFO     	| Interval 2 (refinement training and applying) time 20.15546 s
2023-12-19 23:50:23 INFO     | Time for this section (main program): 32.43039 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.918878078460693
The empirical coverage with aps is: 0.8915845155715942
The empirical efficiency with daps is: 4.917361736297607
The empirical coverage with daps is: 0.8809704184532166
2023-12-19 23:50:26.396176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:50:35 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.7, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:50:35 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:50:35 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:50:35 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:50:35 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:50:35 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:50:35 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:50:35 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:50:35 INFO     	| Interval 0 (graph coarsening) time 0.24269 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:50:35 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:50:45 INFO     	| Interval 1 (embedding) time 10.39180 s
2023-12-19 23:50:45 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:50:45 INFO     initial_embed: (1518, 128)
2023-12-19 23:50:45 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.1288213729858398, MSE Loss: 0.4903576374053955, ACC loss: 1.4024486541748047
Epoch 200, Loss: 0.9058136940002441, MSE Loss: 0.476620078086853, ACC loss: 1.0897537469863892
Epoch 300, Loss: 0.803756833076477, MSE Loss: 0.46291202306747437, ACC loss: 0.9498332142829895
Epoch 400, Loss: 0.7493064403533936, MSE Loss: 0.4386254847049713, ACC loss: 0.8824553489685059
Epoch 500, Loss: 0.7103607058525085, MSE Loss: 0.415137380361557, ACC loss: 0.8368849754333496
2023-12-19 23:51:07 INFO     		| Interval 1 (training the model) time 22.10877 s
2023-12-19 23:51:07 INFO     			Refinement at level 3 completed.
2023-12-19 23:51:07 INFO     			Refinement at level 2 completed.
2023-12-19 23:51:07 INFO     			Refinement at level 1 completed.
2023-12-19 23:51:07 INFO     		| Interval 2 (refinement) time 0.05995 s
2023-12-19 23:51:07 INFO     	| Interval 2 (refinement training and applying) time 22.17091 s
2023-12-19 23:51:07 INFO     | Time for this section (main program): 32.80541 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.110690116882324
The empirical coverage with aps is: 0.8984078764915466
The empirical efficiency with daps is: 5.060651779174805
The empirical coverage with daps is: 0.8999241590499878
2023-12-19 23:51:11.148964: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:51:19 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.9, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:51:19 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:51:20 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:51:20 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:51:20 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:51:20 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:51:20 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:51:20 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:51:20 INFO     	| Interval 0 (graph coarsening) time 0.24070 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:51:20 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:51:30 INFO     	| Interval 1 (embedding) time 10.08086 s
2023-12-19 23:51:30 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:51:30 INFO     initial_embed: (1518, 128)
2023-12-19 23:51:30 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6104286909103394, MSE Loss: 0.8572307229042053, ACC loss: 0.5830062627792358
Epoch 200, Loss: 0.3511154353618622, MSE Loss: 0.8243663907051086, ACC loss: 0.29853197932243347
Epoch 300, Loss: 0.2452503740787506, MSE Loss: 0.8071478009223938, ACC loss: 0.18281733989715576
Epoch 400, Loss: 0.2031354010105133, MSE Loss: 0.7489352822303772, ACC loss: 0.14249098300933838
Epoch 500, Loss: 0.16393113136291504, MSE Loss: 0.7114524841308594, ACC loss: 0.10309543460607529
2023-12-19 23:51:50 INFO     		| Interval 1 (training the model) time 20.05022 s
2023-12-19 23:51:50 INFO     			Refinement at level 3 completed.
2023-12-19 23:51:50 INFO     			Refinement at level 2 completed.
2023-12-19 23:51:50 INFO     			Refinement at level 1 completed.
2023-12-19 23:51:50 INFO     		| Interval 2 (refinement) time 0.05314 s
2023-12-19 23:51:50 INFO     	| Interval 2 (refinement training and applying) time 20.10490 s
2023-12-19 23:51:50 INFO     | Time for this section (main program): 30.42646 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.056861400604248
The empirical coverage with aps is: 0.8756633996963501
The empirical efficiency with daps is: 5.141016006469727
The empirical coverage with daps is: 0.8817285895347595
2023-12-19 23:51:53.072391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:52:01 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.9, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:52:01 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:52:01 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:52:02 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:52:02 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:52:02 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:52:02 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:52:02 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:52:02 INFO     	| Interval 0 (graph coarsening) time 0.24272 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:52:02 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:52:12 INFO     	| Interval 1 (embedding) time 10.67155 s
2023-12-19 23:52:12 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:52:12 INFO     initial_embed: (1518, 128)
2023-12-19 23:52:12 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7354757785797119, MSE Loss: 0.8431463837623596, ACC loss: 0.7235124111175537
Epoch 200, Loss: 0.6340276002883911, MSE Loss: 0.7563516497612, ACC loss: 0.6204360127449036
Epoch 300, Loss: 0.5232428908348083, MSE Loss: 0.7705702781677246, ACC loss: 0.4957621097564697
Epoch 400, Loss: 0.41262856125831604, MSE Loss: 0.8052833676338196, ACC loss: 0.36900025606155396
Epoch 500, Loss: 0.31458282470703125, MSE Loss: 0.7904490828514099, ACC loss: 0.2617088258266449
2023-12-19 23:52:32 INFO     		| Interval 1 (training the model) time 19.48604 s
2023-12-19 23:52:32 INFO     			Refinement at level 3 completed.
2023-12-19 23:52:32 INFO     			Refinement at level 2 completed.
2023-12-19 23:52:32 INFO     			Refinement at level 1 completed.
2023-12-19 23:52:32 INFO     		| Interval 2 (refinement) time 0.05585 s
2023-12-19 23:52:32 INFO     	| Interval 2 (refinement training and applying) time 19.54406 s
2023-12-19 23:52:32 INFO     | Time for this section (main program): 30.45834 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.174374580383301
The empirical coverage with aps is: 0.9105383157730103
The empirical efficiency with daps is: 5.181955814361572
The empirical coverage with daps is: 0.9150871634483337
2023-12-19 23:52:35.010629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:52:43 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.9, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:52:43 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:52:43 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:52:43 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:52:43 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:52:43 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:52:43 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:52:43 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:52:43 INFO     	| Interval 0 (graph coarsening) time 0.24568 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:52:43 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:52:54 INFO     	| Interval 1 (embedding) time 10.97817 s
2023-12-19 23:52:54 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:52:54 INFO     initial_embed: (1518, 128)
2023-12-19 23:52:54 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7970877289772034, MSE Loss: 0.935273289680481, ACC loss: 0.7817338109016418
Epoch 200, Loss: 0.7161613702774048, MSE Loss: 0.750617504119873, ACC loss: 0.7123329639434814
Epoch 300, Loss: 0.6488402485847473, MSE Loss: 0.8056240677833557, ACC loss: 0.631419837474823
Epoch 400, Loss: 0.6169394254684448, MSE Loss: 0.766990602016449, ACC loss: 0.6002670526504517
Epoch 500, Loss: 0.5850536823272705, MSE Loss: 0.7387043833732605, ACC loss: 0.5679814219474792
2023-12-19 23:53:16 INFO     		| Interval 1 (training the model) time 21.20412 s
2023-12-19 23:53:16 INFO     			Refinement at level 3 completed.
2023-12-19 23:53:16 INFO     			Refinement at level 2 completed.
2023-12-19 23:53:16 INFO     			Refinement at level 1 completed.
2023-12-19 23:53:16 INFO     		| Interval 2 (refinement) time 0.06047 s
2023-12-19 23:53:16 INFO     	| Interval 2 (refinement training and applying) time 21.26623 s
2023-12-19 23:53:16 INFO     | Time for this section (main program): 32.49008 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.346474647521973
The empirical coverage with aps is: 0.9097801446914673
The empirical efficiency with daps is: 5.346474647521973
The empirical coverage with daps is: 0.9105383157730103
2023-12-19 23:53:19.417213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:53:27 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.9, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:53:27 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:53:28 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:53:28 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:53:28 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:53:28 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:53:28 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:53:28 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:53:28 INFO     	| Interval 0 (graph coarsening) time 0.24257 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:53:28 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:53:39 INFO     	| Interval 1 (embedding) time 11.04074 s
2023-12-19 23:53:39 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:53:39 INFO     initial_embed: (1518, 128)
2023-12-19 23:53:39 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.1516804695129395, MSE Loss: 1.0479514598846436, ACC loss: 1.1632059812545776
Epoch 200, Loss: 0.9202082753181458, MSE Loss: 1.1226177215576172, ACC loss: 0.8977183699607849
Epoch 300, Loss: 0.8411060571670532, MSE Loss: 1.0984140634536743, ACC loss: 0.8125163316726685
Epoch 400, Loss: 0.7970377206802368, MSE Loss: 1.0474039316177368, ACC loss: 0.7692192792892456
Epoch 500, Loss: 0.7641135454177856, MSE Loss: 1.0240793228149414, ACC loss: 0.7352284789085388
2023-12-19 23:54:00 INFO     		| Interval 1 (training the model) time 21.03578 s
2023-12-19 23:54:00 INFO     			Refinement at level 3 completed.
2023-12-19 23:54:00 INFO     			Refinement at level 2 completed.
2023-12-19 23:54:00 INFO     			Refinement at level 1 completed.
2023-12-19 23:54:00 INFO     		| Interval 2 (refinement) time 0.04933 s
2023-12-19 23:54:00 INFO     	| Interval 2 (refinement training and applying) time 21.08681 s
2023-12-19 23:54:00 INFO     | Time for this section (main program): 32.37012 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.424563884735107
The empirical coverage with aps is: 0.9120545983314514
The empirical efficiency with daps is: 5.440485000610352
The empirical coverage with daps is: 0.9150871634483337
2023-12-19 23:54:03.242436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:54:12 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.99, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:54:12 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:54:12 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:54:12 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:54:12 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:54:12 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:54:12 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:54:12 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:54:12 INFO     	| Interval 0 (graph coarsening) time 0.24335 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:54:12 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:54:23 INFO     	| Interval 1 (embedding) time 10.96622 s
2023-12-19 23:54:23 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:54:23 INFO     initial_embed: (1518, 128)
2023-12-19 23:54:23 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6961680054664612, MSE Loss: 1.4022446870803833, ACC loss: 0.6890358924865723
Epoch 200, Loss: 0.35991618037223816, MSE Loss: 1.5426065921783447, ACC loss: 0.34796980023384094
Epoch 300, Loss: 0.365251749753952, MSE Loss: 1.4831972122192383, ACC loss: 0.3539593815803528
Epoch 400, Loss: 0.2234557718038559, MSE Loss: 1.5345571041107178, ACC loss: 0.2102123200893402
Epoch 500, Loss: 0.11926432698965073, MSE Loss: 1.5950543880462646, ACC loss: 0.10435735434293747
2023-12-19 23:54:43 INFO     		| Interval 1 (training the model) time 20.24966 s
2023-12-19 23:54:43 INFO     			Refinement at level 3 completed.
2023-12-19 23:54:43 INFO     			Refinement at level 2 completed.
2023-12-19 23:54:43 INFO     			Refinement at level 1 completed.
2023-12-19 23:54:43 INFO     		| Interval 2 (refinement) time 0.05129 s
2023-12-19 23:54:43 INFO     	| Interval 2 (refinement training and applying) time 20.30300 s
2023-12-19 23:54:43 INFO     | Time for this section (main program): 31.51257 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.190295696258545
The empirical coverage with aps is: 0.8999241590499878
The empirical efficiency with daps is: 5.192570209503174
The empirical coverage with daps is: 0.9021986126899719
2023-12-19 23:54:46.404657: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:54:55 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.99, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:54:55 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:54:55 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:54:55 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:54:55 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:54:55 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:54:55 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:54:55 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:54:55 INFO     	| Interval 0 (graph coarsening) time 0.24387 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:54:55 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:55:09 INFO     	| Interval 1 (embedding) time 13.63911 s
2023-12-19 23:55:09 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:55:09 INFO     initial_embed: (1518, 128)
2023-12-19 23:55:09 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.809931218624115, MSE Loss: 1.3641892671585083, ACC loss: 0.8043326735496521
Epoch 200, Loss: 0.6412861347198486, MSE Loss: 1.4052295684814453, ACC loss: 0.6335695385932922
Epoch 300, Loss: 0.5813776850700378, MSE Loss: 1.408732533454895, ACC loss: 0.5730205774307251
Epoch 400, Loss: 0.3929901123046875, MSE Loss: 1.4682190418243408, ACC loss: 0.3821291923522949
Epoch 500, Loss: 0.3178246021270752, MSE Loss: 1.440137267112732, ACC loss: 0.3064880967140198
2023-12-19 23:55:31 INFO     		| Interval 1 (training the model) time 22.38726 s
2023-12-19 23:55:31 INFO     			Refinement at level 3 completed.
2023-12-19 23:55:31 INFO     			Refinement at level 2 completed.
2023-12-19 23:55:31 INFO     			Refinement at level 1 completed.
2023-12-19 23:55:31 INFO     		| Interval 2 (refinement) time 0.05165 s
2023-12-19 23:55:31 INFO     	| Interval 2 (refinement training and applying) time 22.44144 s
2023-12-19 23:55:31 INFO     | Time for this section (main program): 36.32442 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.463229656219482
The empirical coverage with aps is: 0.9143290519714355
The empirical efficiency with daps is: 5.496588230133057
The empirical coverage with daps is: 0.9128127098083496
2023-12-19 23:55:34.389301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:55:43 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.99, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:55:43 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:55:43 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:55:43 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:55:43 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:55:43 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:55:43 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:55:43 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:55:43 INFO     	| Interval 0 (graph coarsening) time 0.24404 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:55:43 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:55:55 INFO     	| Interval 1 (embedding) time 12.36265 s
2023-12-19 23:55:55 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:55:55 INFO     initial_embed: (1518, 128)
2023-12-19 23:55:55 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.8237196803092957, MSE Loss: 1.5421478748321533, ACC loss: 0.8164628744125366
Epoch 200, Loss: 0.7228071093559265, MSE Loss: 1.5459717512130737, ACC loss: 0.7144923210144043
Epoch 300, Loss: 0.6477882266044617, MSE Loss: 1.5535597801208496, ACC loss: 0.6386390328407288
Epoch 400, Loss: 0.6207777261734009, MSE Loss: 1.5398293733596802, ACC loss: 0.6114943623542786
Epoch 500, Loss: 0.5909993648529053, MSE Loss: 1.5651236772537231, ACC loss: 0.5811597108840942
2023-12-19 23:56:16 INFO     		| Interval 1 (training the model) time 20.45578 s
2023-12-19 23:56:16 INFO     			Refinement at level 3 completed.
2023-12-19 23:56:16 INFO     			Refinement at level 2 completed.
2023-12-19 23:56:16 INFO     			Refinement at level 1 completed.
2023-12-19 23:56:16 INFO     		| Interval 2 (refinement) time 0.05572 s
2023-12-19 23:56:16 INFO     	| Interval 2 (refinement training and applying) time 20.51291 s
2023-12-19 23:56:16 INFO     | Time for this section (main program): 33.11960 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.344958305358887
The empirical coverage with aps is: 0.8999241590499878
The empirical efficiency with daps is: 5.325246334075928
The empirical coverage with daps is: 0.9082638621330261
2023-12-19 23:56:19.065208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:56:27 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=500, jobid=92499, lambda_fl=0.99, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:56:27 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:56:28 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:56:28 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:56:28 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:56:28 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:56:28 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:56:28 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:56:28 INFO     	| Interval 0 (graph coarsening) time 0.24216 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:56:28 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:56:40 INFO     	| Interval 1 (embedding) time 12.81279 s
2023-12-19 23:56:40 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:56:41 INFO     initial_embed: (1518, 128)
2023-12-19 23:56:41 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.0992155075073242, MSE Loss: 1.765914797782898, ACC loss: 1.0924811363220215
Epoch 200, Loss: 0.8918701410293579, MSE Loss: 1.7924312353134155, ACC loss: 0.8827735781669617
Epoch 300, Loss: 0.8142181634902954, MSE Loss: 1.774753451347351, ACC loss: 0.8045157790184021
Epoch 400, Loss: 0.7711703181266785, MSE Loss: 1.7482588291168213, ACC loss: 0.7613007426261902
Epoch 500, Loss: 0.7815701961517334, MSE Loss: 1.7063590288162231, ACC loss: 0.7722288370132446
2023-12-19 23:57:02 INFO     		| Interval 1 (training the model) time 21.18003 s
2023-12-19 23:57:02 INFO     			Refinement at level 3 completed.
2023-12-19 23:57:02 INFO     			Refinement at level 2 completed.
2023-12-19 23:57:02 INFO     			Refinement at level 1 completed.
2023-12-19 23:57:02 INFO     		| Interval 2 (refinement) time 0.06188 s
2023-12-19 23:57:02 INFO     	| Interval 2 (refinement training and applying) time 21.24346 s
2023-12-19 23:57:02 INFO     | Time for this section (main program): 34.29840 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.115996837615967
The empirical coverage with aps is: 0.904473066329956
The empirical efficiency with daps is: 5.144806861877441
The empirical coverage with daps is: 0.9105383157730103
2023-12-19 23:57:04.969687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:57:13 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.1, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:57:13 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:57:14 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:57:14 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:57:14 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:57:14 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:57:14 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:57:14 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:57:14 INFO     	| Interval 0 (graph coarsening) time 0.24525 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:57:14 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:57:25 INFO     	| Interval 1 (embedding) time 11.61545 s
2023-12-19 23:57:25 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-19 23:57:25 INFO     initial_embed: (1518, 128)
2023-12-19 23:57:25 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.2388133406639099, MSE Loss: 0.13274380564689636, ACC loss: 1.193439245223999
Epoch 200, Loss: 0.16361989080905914, MSE Loss: 0.07561346888542175, ACC loss: 0.9556776881217957
Epoch 300, Loss: 0.13713756203651428, MSE Loss: 0.0553336925804615, ACC loss: 0.873372495174408
Epoch 400, Loss: 0.1474866420030594, MSE Loss: 0.06085382029414177, ACC loss: 0.9271820187568665
Epoch 500, Loss: 0.10928872227668762, MSE Loss: 0.04564269632101059, ACC loss: 0.6821029782295227
Epoch 600, Loss: 0.09521284699440002, MSE Loss: 0.044461362063884735, ACC loss: 0.5519762635231018
Epoch 700, Loss: 0.08552469313144684, MSE Loss: 0.044027697294950485, ACC loss: 0.4589976370334625
Epoch 800, Loss: 0.07811783254146576, MSE Loss: 0.04310673847794533, ACC loss: 0.39321771264076233
Epoch 900, Loss: 0.07262612879276276, MSE Loss: 0.04176066070795059, ACC loss: 0.35041528940200806
Epoch 1000, Loss: 0.07190316915512085, MSE Loss: 0.04186616837978363, ACC loss: 0.3422362208366394
2023-12-19 23:58:07 INFO     		| Interval 1 (training the model) time 42.20279 s
2023-12-19 23:58:07 INFO     			Refinement at level 3 completed.
2023-12-19 23:58:07 INFO     			Refinement at level 2 completed.
2023-12-19 23:58:07 INFO     			Refinement at level 1 completed.
2023-12-19 23:58:07 INFO     		| Interval 2 (refinement) time 0.06784 s
2023-12-19 23:58:07 INFO     	| Interval 2 (refinement training and applying) time 42.27261 s
2023-12-19 23:58:07 INFO     | Time for this section (main program): 54.13330 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.297953128814697
The empirical coverage with aps is: 0.9211524128913879
The empirical efficiency with daps is: 5.269901275634766
The empirical coverage with daps is: 0.9188779592514038
2023-12-19 23:58:11.321271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:58:20 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.1, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:58:20 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:58:20 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:58:20 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:58:20 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:58:20 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:58:20 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:58:20 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:58:20 INFO     	| Interval 0 (graph coarsening) time 0.24347 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:58:20 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:58:32 INFO     	| Interval 1 (embedding) time 12.41743 s
2023-12-19 23:58:32 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:58:33 INFO     initial_embed: (1518, 128)
2023-12-19 23:58:33 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.20957961678504944, MSE Loss: 0.09525439888238907, ACC loss: 1.2385066747665405
Epoch 200, Loss: 0.15224501490592957, MSE Loss: 0.05829375237226486, ACC loss: 0.9978063702583313
Epoch 300, Loss: 0.1323249489068985, MSE Loss: 0.04462439566850662, ACC loss: 0.9216299057006836
Epoch 400, Loss: 0.12029631435871124, MSE Loss: 0.039954282343387604, ACC loss: 0.8433746099472046
Epoch 500, Loss: 0.11770391464233398, MSE Loss: 0.03999839723110199, ACC loss: 0.8170535564422607
Epoch 600, Loss: 0.11206729710102081, MSE Loss: 0.037221334874629974, ACC loss: 0.7856810092926025
Epoch 700, Loss: 0.10586529970169067, MSE Loss: 0.03831830620765686, ACC loss: 0.7137881517410278
Epoch 800, Loss: 0.09702065587043762, MSE Loss: 0.037615519016981125, ACC loss: 0.631666898727417
Epoch 900, Loss: 0.09207405149936676, MSE Loss: 0.037254221737384796, ACC loss: 0.5854524970054626
Epoch 1000, Loss: 0.08629751950502396, MSE Loss: 0.03775377571582794, ACC loss: 0.5231912136077881
2023-12-19 23:59:15 INFO     		| Interval 1 (training the model) time 42.33043 s
2023-12-19 23:59:15 INFO     			Refinement at level 3 completed.
2023-12-19 23:59:15 INFO     			Refinement at level 2 completed.
2023-12-19 23:59:15 INFO     			Refinement at level 1 completed.
2023-12-19 23:59:15 INFO     		| Interval 2 (refinement) time 0.05465 s
2023-12-19 23:59:15 INFO     	| Interval 2 (refinement training and applying) time 42.38754 s
2023-12-19 23:59:15 INFO     | Time for this section (main program): 55.04843 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.1228203773498535
The empirical coverage with aps is: 0.9112964272499084
The empirical efficiency with daps is: 5.203184127807617
The empirical coverage with daps is: 0.9128127098083496
2023-12-19 23:59:18.142799: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-19 23:59:26 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.1, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-19 23:59:26 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-19 23:59:26 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-19 23:59:26 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-19 23:59:26 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-19 23:59:26 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-19 23:59:26 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-19 23:59:26 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-19 23:59:26 INFO     	| Interval 0 (graph coarsening) time 0.25008 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-19 23:59:27 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-19 23:59:41 INFO     	| Interval 1 (embedding) time 14.43904 s
2023-12-19 23:59:41 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-19 23:59:41 INFO     initial_embed: (1518, 128)
2023-12-19 23:59:41 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.23459230363368988, MSE Loss: 0.10649621486663818, ACC loss: 1.387457013130188
Epoch 200, Loss: 0.16534362733364105, MSE Loss: 0.05418086051940918, ACC loss: 1.1658085584640503
Epoch 300, Loss: 0.1440563201904297, MSE Loss: 0.042679525911808014, ACC loss: 1.0564475059509277
Epoch 400, Loss: 0.13424555957317352, MSE Loss: 0.03931240737438202, ACC loss: 0.988644003868103
Epoch 500, Loss: 0.1266012042760849, MSE Loss: 0.036482855677604675, ACC loss: 0.9376662969589233
Epoch 600, Loss: 0.12192990630865097, MSE Loss: 0.03534795716404915, ACC loss: 0.9011674523353577
Epoch 700, Loss: 0.11919397115707397, MSE Loss: 0.03469457849860191, ACC loss: 0.8796885013580322
Epoch 800, Loss: 0.1187710165977478, MSE Loss: 0.03497440740466118, ACC loss: 0.8729404807090759
Epoch 900, Loss: 0.1133018359541893, MSE Loss: 0.03361240774393082, ACC loss: 0.8305066823959351
Epoch 1000, Loss: 0.11018221825361252, MSE Loss: 0.03357283025979996, ACC loss: 0.799666702747345
2023-12-20 00:00:25 INFO     		| Interval 1 (training the model) time 43.96253 s
2023-12-20 00:00:25 INFO     			Refinement at level 3 completed.
2023-12-20 00:00:25 INFO     			Refinement at level 2 completed.
2023-12-20 00:00:25 INFO     			Refinement at level 1 completed.
2023-12-20 00:00:25 INFO     		| Interval 2 (refinement) time 0.06403 s
2023-12-20 00:00:25 INFO     	| Interval 2 (refinement training and applying) time 44.02864 s
2023-12-20 00:00:25 INFO     | Time for this section (main program): 58.71775 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 4.985595226287842
The empirical coverage with aps is: 0.8786959648132324
The empirical efficiency with daps is: 5.024260997772217
The empirical coverage with daps is: 0.8847611546516418
2023-12-20 00:00:28.597875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:00:37 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.1, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:00:37 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:00:37 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:00:37 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:00:37 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:00:37 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:00:37 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:00:37 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:00:37 INFO     	| Interval 0 (graph coarsening) time 0.24199 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:00:37 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:00:50 INFO     	| Interval 1 (embedding) time 12.35176 s
2023-12-20 00:00:50 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:00:50 INFO     initial_embed: (1518, 128)
2023-12-20 00:00:50 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.45398378372192383, MSE Loss: 0.3178776204586029, ACC loss: 1.6789395809173584
Epoch 200, Loss: 0.289275586605072, MSE Loss: 0.14703825116157532, ACC loss: 1.5694117546081543
Epoch 300, Loss: 0.2302665114402771, MSE Loss: 0.09101054817438126, ACC loss: 1.4835702180862427
Epoch 400, Loss: 0.20013263821601868, MSE Loss: 0.06538031250238419, ACC loss: 1.4129035472869873
Epoch 500, Loss: 0.18251411616802216, MSE Loss: 0.05265360698103905, ACC loss: 1.3512587547302246
Epoch 600, Loss: 0.17086102068424225, MSE Loss: 0.045671526342630386, ACC loss: 1.2975664138793945
Epoch 700, Loss: 0.16258284449577332, MSE Loss: 0.041612766683101654, ACC loss: 1.2513134479522705
Epoch 800, Loss: 0.15647658705711365, MSE Loss: 0.03924808278679848, ACC loss: 1.2115330696105957
Epoch 900, Loss: 0.15176525712013245, MSE Loss: 0.037824563682079315, ACC loss: 1.1772315502166748
Epoch 1000, Loss: 0.14795847237110138, MSE Loss: 0.03690994903445244, ACC loss: 1.147395133972168
2023-12-20 00:01:32 INFO     		| Interval 1 (training the model) time 41.90836 s
2023-12-20 00:01:32 INFO     			Refinement at level 3 completed.
2023-12-20 00:01:32 INFO     			Refinement at level 2 completed.
2023-12-20 00:01:32 INFO     			Refinement at level 1 completed.
2023-12-20 00:01:32 INFO     		| Interval 2 (refinement) time 0.04790 s
2023-12-20 00:01:32 INFO     	| Interval 2 (refinement training and applying) time 41.95805 s
2023-12-20 00:01:32 INFO     | Time for this section (main program): 54.55181 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.128127574920654
The empirical coverage with aps is: 0.9150871634483337
The empirical efficiency with daps is: 5.034116744995117
The empirical coverage with daps is: 0.9029567837715149
2023-12-20 00:01:35.479761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:01:44 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.3, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:01:44 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:01:44 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:01:44 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:01:44 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:01:44 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:01:44 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:01:44 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:01:44 INFO     	| Interval 0 (graph coarsening) time 0.24297 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:01:44 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:01:56 INFO     	| Interval 1 (embedding) time 11.76844 s
2023-12-20 00:01:56 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:01:56 INFO     initial_embed: (1518, 128)
2023-12-20 00:01:56 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.3969115912914276, MSE Loss: 0.1644832193851471, ACC loss: 0.9392443895339966
Epoch 200, Loss: 0.2916069030761719, MSE Loss: 0.1319877654314041, ACC loss: 0.6640514731407166
Epoch 300, Loss: 0.22000481188297272, MSE Loss: 0.11275289952754974, ACC loss: 0.47025927901268005
Epoch 400, Loss: 0.17056332528591156, MSE Loss: 0.10341137647628784, ACC loss: 0.3272511959075928
Epoch 500, Loss: 0.21069486439228058, MSE Loss: 0.10369192808866501, ACC loss: 0.46036839485168457
Epoch 600, Loss: 0.14594462513923645, MSE Loss: 0.096376433968544, ACC loss: 0.2616037130355835
Epoch 700, Loss: 0.12536565959453583, MSE Loss: 0.08539074659347534, ACC loss: 0.21864043176174164
Epoch 800, Loss: 0.1259286105632782, MSE Loss: 0.09023231267929077, ACC loss: 0.20921997725963593
Epoch 900, Loss: 0.2420414686203003, MSE Loss: 0.10679703950881958, ACC loss: 0.5576118230819702
Epoch 1000, Loss: 0.10598918795585632, MSE Loss: 0.07688459753990173, ACC loss: 0.17389990389347076
2023-12-20 00:02:36 INFO     		| Interval 1 (training the model) time 40.35460 s
2023-12-20 00:02:36 INFO     			Refinement at level 3 completed.
2023-12-20 00:02:36 INFO     			Refinement at level 2 completed.
2023-12-20 00:02:36 INFO     			Refinement at level 1 completed.
2023-12-20 00:02:36 INFO     		| Interval 2 (refinement) time 0.05080 s
2023-12-20 00:02:36 INFO     	| Interval 2 (refinement training and applying) time 40.40683 s
2023-12-20 00:02:36 INFO     | Time for this section (main program): 52.41824 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 6.0
The empirical coverage with aps is: 1.0
The empirical efficiency with daps is: 5.313116073608398
The empirical coverage with daps is: 0.9211524128913879
2023-12-20 00:02:39.580712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:02:48 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.3, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:02:48 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:02:48 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:02:48 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:02:48 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:02:48 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:02:48 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:02:48 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:02:48 INFO     	| Interval 0 (graph coarsening) time 0.24286 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:02:48 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:03:01 INFO     	| Interval 1 (embedding) time 12.25281 s
2023-12-20 00:03:01 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-20 00:03:01 INFO     initial_embed: (1518, 128)
2023-12-20 00:03:01 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.41578665375709534, MSE Loss: 0.13881048560142517, ACC loss: 1.062064290046692
Epoch 200, Loss: 0.3390842378139496, MSE Loss: 0.09903509169816971, ACC loss: 0.8991988897323608
Epoch 300, Loss: 0.29667365550994873, MSE Loss: 0.087877057492733, ACC loss: 0.7838656902313232
Epoch 400, Loss: 0.28017252683639526, MSE Loss: 0.09345464408397675, ACC loss: 0.7158475518226624
Epoch 500, Loss: 0.22328558564186096, MSE Loss: 0.09032826870679855, ACC loss: 0.5335193276405334
Epoch 600, Loss: 0.19652718305587769, MSE Loss: 0.08669370412826538, ACC loss: 0.4528052806854248
Epoch 700, Loss: 0.17259135842323303, MSE Loss: 0.08437643945217133, ACC loss: 0.3784261643886566
Epoch 800, Loss: 0.15905307233333588, MSE Loss: 0.08206639438867569, ACC loss: 0.3386886417865753
Epoch 900, Loss: 0.14874446392059326, MSE Loss: 0.07949786633253098, ACC loss: 0.31031984090805054
Epoch 1000, Loss: 0.13729499280452728, MSE Loss: 0.07526165246963501, ACC loss: 0.28203943371772766
2023-12-20 00:03:42 INFO     		| Interval 1 (training the model) time 41.76230 s
2023-12-20 00:03:42 INFO     			Refinement at level 3 completed.
2023-12-20 00:03:42 INFO     			Refinement at level 2 completed.
2023-12-20 00:03:42 INFO     			Refinement at level 1 completed.
2023-12-20 00:03:42 INFO     		| Interval 2 (refinement) time 0.05916 s
2023-12-20 00:03:42 INFO     	| Interval 2 (refinement training and applying) time 41.82347 s
2023-12-20 00:03:42 INFO     | Time for this section (main program): 54.31914 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 6.0
The empirical coverage with aps is: 1.0
The empirical efficiency with daps is: 5.205458641052246
The empirical coverage with daps is: 0.9150871634483337
2023-12-20 00:03:45.721823: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:03:54 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.3, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:03:54 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:03:54 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:03:54 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:03:54 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:03:54 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:03:54 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:03:54 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:03:54 INFO     	| Interval 0 (graph coarsening) time 0.24240 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:03:54 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:04:07 INFO     	| Interval 1 (embedding) time 12.41771 s
2023-12-20 00:04:07 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:04:07 INFO     initial_embed: (1518, 128)
2023-12-20 00:04:07 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.4653506875038147, MSE Loss: 0.14856450259685516, ACC loss: 1.204518437385559
Epoch 200, Loss: 0.3634101152420044, MSE Loss: 0.10797444730997086, ACC loss: 0.9594266414642334
Epoch 300, Loss: 0.35055306553840637, MSE Loss: 0.09874564409255981, ACC loss: 0.9381036758422852
Epoch 400, Loss: 0.3083079159259796, MSE Loss: 0.08600844442844391, ACC loss: 0.8270066380500793
Epoch 500, Loss: 0.29187172651290894, MSE Loss: 0.08150731772184372, ACC loss: 0.782721996307373
Epoch 600, Loss: 0.27367928624153137, MSE Loss: 0.07817963510751724, ACC loss: 0.7298451066017151
Epoch 700, Loss: 0.2786635458469391, MSE Loss: 0.07711127400398254, ACC loss: 0.7489521503448486
Epoch 800, Loss: 0.2435680627822876, MSE Loss: 0.07778508961200714, ACC loss: 0.6303949952125549
Epoch 900, Loss: 0.23699457943439484, MSE Loss: 0.07567764073610306, ACC loss: 0.6134007573127747
Epoch 1000, Loss: 0.2126595377922058, MSE Loss: 0.07796110212802887, ACC loss: 0.5269558429718018
2023-12-20 00:04:49 INFO     		| Interval 1 (training the model) time 42.07244 s
2023-12-20 00:04:49 INFO     			Refinement at level 3 completed.
2023-12-20 00:04:49 INFO     			Refinement at level 2 completed.
2023-12-20 00:04:49 INFO     			Refinement at level 1 completed.
2023-12-20 00:04:49 INFO     		| Interval 2 (refinement) time 0.06073 s
2023-12-20 00:04:49 INFO     	| Interval 2 (refinement training and applying) time 42.13466 s
2023-12-20 00:04:49 INFO     | Time for this section (main program): 54.79477 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.321455478668213
The empirical coverage with aps is: 0.9287338852882385
The empirical efficiency with daps is: 5.212282180786133
The empirical coverage with daps is: 0.9029567837715149
2023-12-20 00:04:52.675218: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:05:01 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.3, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:05:01 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:05:01 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:05:02 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:05:02 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:05:02 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:05:02 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:05:02 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:05:02 INFO     	| Interval 0 (graph coarsening) time 0.24461 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:05:02 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:05:14 INFO     	| Interval 1 (embedding) time 12.42155 s
2023-12-20 00:05:14 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:05:14 INFO     initial_embed: (1518, 128)
2023-12-20 00:05:14 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7198318243026733, MSE Loss: 0.3417057693004608, ACC loss: 1.6021257638931274
Epoch 200, Loss: 0.5524755716323853, MSE Loss: 0.1725626140832901, ACC loss: 1.4389389753341675
Epoch 300, Loss: 0.47586873173713684, MSE Loss: 0.1267661452293396, ACC loss: 1.2904413938522339
Epoch 400, Loss: 0.4295434057712555, MSE Loss: 0.10844922065734863, ACC loss: 1.1787631511688232
Epoch 500, Loss: 0.39978739619255066, MSE Loss: 0.09937009960412979, ACC loss: 1.10076105594635
Epoch 600, Loss: 0.3790998160839081, MSE Loss: 0.09385594725608826, ACC loss: 1.0446687936782837
Epoch 700, Loss: 0.36375322937965393, MSE Loss: 0.09024474769830704, ACC loss: 1.0019396543502808
Epoch 800, Loss: 0.35279032588005066, MSE Loss: 0.08820372074842453, ACC loss: 0.9701589941978455
Epoch 900, Loss: 0.3415687382221222, MSE Loss: 0.08548489212989807, ACC loss: 0.9390976428985596
Epoch 1000, Loss: 0.3327654004096985, MSE Loss: 0.08376007527112961, ACC loss: 0.913777768611908
2023-12-20 00:05:55 INFO     		| Interval 1 (training the model) time 40.67589 s
2023-12-20 00:05:55 INFO     			Refinement at level 3 completed.
2023-12-20 00:05:55 INFO     			Refinement at level 2 completed.
2023-12-20 00:05:55 INFO     			Refinement at level 1 completed.
2023-12-20 00:05:55 INFO     		| Interval 2 (refinement) time 0.05677 s
2023-12-20 00:05:55 INFO     	| Interval 2 (refinement training and applying) time 40.73421 s
2023-12-20 00:05:55 INFO     | Time for this section (main program): 53.40037 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.001516342163086
The empirical coverage with aps is: 0.8824867606163025
The empirical efficiency with daps is: 5.003790855407715
The empirical coverage with daps is: 0.8961334228515625
2023-12-20 00:05:57.880340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:06:06 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.5, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:06:06 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:06:07 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:06:07 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:06:07 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:06:07 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:06:07 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:06:07 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:06:07 INFO     	| Interval 0 (graph coarsening) time 0.24156 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:06:07 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:06:18 INFO     	| Interval 1 (embedding) time 11.79680 s
2023-12-20 00:06:18 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:06:19 INFO     initial_embed: (1518, 128)
2023-12-20 00:06:19 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.5688188076019287, MSE Loss: 0.2895217537879944, ACC loss: 0.8481159210205078
Epoch 200, Loss: 0.38330450654029846, MSE Loss: 0.2471901774406433, ACC loss: 0.5194188356399536
Epoch 300, Loss: 0.3532542884349823, MSE Loss: 0.2403893619775772, ACC loss: 0.4661192297935486
Epoch 400, Loss: 0.2390555739402771, MSE Loss: 0.20922045409679413, ACC loss: 0.2688906788825989
Epoch 500, Loss: 0.21042555570602417, MSE Loss: 0.19397249817848206, ACC loss: 0.22687862813472748
Epoch 600, Loss: 0.38308191299438477, MSE Loss: 0.21017025411128998, ACC loss: 0.5559935569763184
Epoch 700, Loss: 0.18810676038265228, MSE Loss: 0.18456099927425385, ACC loss: 0.19165252149105072
Epoch 800, Loss: 0.2792576849460602, MSE Loss: 0.220871701836586, ACC loss: 0.33764365315437317
Epoch 900, Loss: 0.14382536709308624, MSE Loss: 0.16751377284526825, ACC loss: 0.12013696879148483
Epoch 1000, Loss: 0.144540935754776, MSE Loss: 0.17337840795516968, ACC loss: 0.11570347845554352
2023-12-20 00:07:00 INFO     		| Interval 1 (training the model) time 42.05306 s
2023-12-20 00:07:01 INFO     			Refinement at level 3 completed.
2023-12-20 00:07:01 INFO     			Refinement at level 2 completed.
2023-12-20 00:07:01 INFO     			Refinement at level 1 completed.
2023-12-20 00:07:01 INFO     		| Interval 2 (refinement) time 0.05229 s
2023-12-20 00:07:01 INFO     	| Interval 2 (refinement training and applying) time 42.10732 s
2023-12-20 00:07:01 INFO     | Time for this section (main program): 54.14568 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.218347072601318
The empirical coverage with aps is: 0.904473066329956
The empirical efficiency with daps is: 5.222137928009033
The empirical coverage with daps is: 0.904473066329956
2023-12-20 00:07:04.352938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:07:13 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.5, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:07:13 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:07:13 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:07:13 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:07:13 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:07:13 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:07:13 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:07:13 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:07:13 INFO     	| Interval 0 (graph coarsening) time 0.24171 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:07:13 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:07:24 INFO     	| Interval 1 (embedding) time 11.06028 s
2023-12-20 00:07:24 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:07:24 INFO     initial_embed: (1518, 128)
2023-12-20 00:07:24 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.5973659753799438, MSE Loss: 0.21715877950191498, ACC loss: 0.9775732159614563
Epoch 200, Loss: 0.5052124261856079, MSE Loss: 0.18565364181995392, ACC loss: 0.8247712254524231
Epoch 300, Loss: 0.41712871193885803, MSE Loss: 0.16613370180130005, ACC loss: 0.668123722076416
Epoch 400, Loss: 0.38178059458732605, MSE Loss: 0.16144871711730957, ACC loss: 0.6021124720573425
Epoch 500, Loss: 0.29355254769325256, MSE Loss: 0.14937029778957367, ACC loss: 0.43773481249809265
Epoch 600, Loss: 0.25612467527389526, MSE Loss: 0.145848348736763, ACC loss: 0.3664010167121887
Epoch 700, Loss: 0.23835153877735138, MSE Loss: 0.14150333404541016, ACC loss: 0.3351997435092926
Epoch 800, Loss: 0.19335514307022095, MSE Loss: 0.1337311863899231, ACC loss: 0.2529790997505188
Epoch 900, Loss: 0.18235351145267487, MSE Loss: 0.12578454613685608, ACC loss: 0.23892247676849365
Epoch 1000, Loss: 0.16022607684135437, MSE Loss: 0.12239999324083328, ACC loss: 0.19805215299129486
2023-12-20 00:08:06 INFO     		| Interval 1 (training the model) time 41.70789 s
2023-12-20 00:08:06 INFO     			Refinement at level 3 completed.
2023-12-20 00:08:06 INFO     			Refinement at level 2 completed.
2023-12-20 00:08:06 INFO     			Refinement at level 1 completed.
2023-12-20 00:08:06 INFO     		| Interval 2 (refinement) time 0.05524 s
2023-12-20 00:08:06 INFO     	| Interval 2 (refinement training and applying) time 41.76519 s
2023-12-20 00:08:06 INFO     | Time for this section (main program): 53.06718 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 6.0
The empirical coverage with aps is: 1.0
The empirical efficiency with daps is: 6.0
The empirical coverage with daps is: 1.0
2023-12-20 00:08:09.075744: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:08:17 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.5, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:08:17 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:08:18 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:08:18 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:08:18 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:08:18 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:08:18 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:08:18 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:08:18 INFO     	| Interval 0 (graph coarsening) time 0.24322 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:08:18 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:08:28 INFO     	| Interval 1 (embedding) time 10.85905 s
2023-12-20 00:08:28 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:08:29 INFO     initial_embed: (1518, 128)
2023-12-20 00:08:29 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6463789939880371, MSE Loss: 0.23470844328403473, ACC loss: 1.0580495595932007
Epoch 200, Loss: 0.5340867042541504, MSE Loss: 0.19095678627490997, ACC loss: 0.8772165775299072
Epoch 300, Loss: 0.4910680651664734, MSE Loss: 0.176198810338974, ACC loss: 0.8059372901916504
Epoch 400, Loss: 0.4463350176811218, MSE Loss: 0.1580621302127838, ACC loss: 0.7346079349517822
Epoch 500, Loss: 0.4516323208808899, MSE Loss: 0.1590268313884735, ACC loss: 0.7442378401756287
Epoch 600, Loss: 0.34288015961647034, MSE Loss: 0.15311549603939056, ACC loss: 0.5326448082923889
Epoch 700, Loss: 0.33295726776123047, MSE Loss: 0.14632342755794525, ACC loss: 0.5195910930633545
Epoch 800, Loss: 0.33177340030670166, MSE Loss: 0.14848759770393372, ACC loss: 0.515059232711792
Epoch 900, Loss: 0.26077815890312195, MSE Loss: 0.1475481539964676, ACC loss: 0.3740081489086151
Epoch 1000, Loss: 0.23609960079193115, MSE Loss: 0.14740808308124542, ACC loss: 0.3247911334037781
2023-12-20 00:09:09 INFO     		| Interval 1 (training the model) time 40.86644 s
2023-12-20 00:09:09 INFO     			Refinement at level 3 completed.
2023-12-20 00:09:09 INFO     			Refinement at level 2 completed.
2023-12-20 00:09:09 INFO     			Refinement at level 1 completed.
2023-12-20 00:09:09 INFO     		| Interval 2 (refinement) time 0.04991 s
2023-12-20 00:09:09 INFO     	| Interval 2 (refinement training and applying) time 40.91787 s
2023-12-20 00:09:09 INFO     | Time for this section (main program): 52.02014 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.230477809906006
The empirical coverage with aps is: 0.9158453345298767
The empirical efficiency with daps is: 5.1607279777526855
The empirical coverage with daps is: 0.9105383157730103
2023-12-20 00:09:13.119106: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:09:21 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.5, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:09:22 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:09:22 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:09:22 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:09:22 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:09:22 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:09:22 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:09:22 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:09:22 INFO     	| Interval 0 (graph coarsening) time 0.24168 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:09:22 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:09:34 INFO     	| Interval 1 (embedding) time 12.14145 s
2023-12-20 00:09:34 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:09:34 INFO     initial_embed: (1518, 128)
2023-12-20 00:09:34 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.9392156004905701, MSE Loss: 0.3556634485721588, ACC loss: 1.5227677822113037
Epoch 200, Loss: 0.7620797157287598, MSE Loss: 0.24242277443408966, ACC loss: 1.2817366123199463
Epoch 300, Loss: 0.6714128255844116, MSE Loss: 0.21655645966529846, ACC loss: 1.1262692213058472
Epoch 400, Loss: 0.6199997067451477, MSE Loss: 0.20087289810180664, ACC loss: 1.0391265153884888
Epoch 500, Loss: 0.585975170135498, MSE Loss: 0.18889564275741577, ACC loss: 0.9830546379089355
Epoch 600, Loss: 0.5608208775520325, MSE Loss: 0.18013577163219452, ACC loss: 0.941506028175354
Epoch 700, Loss: 0.5399449467658997, MSE Loss: 0.17408831417560577, ACC loss: 0.90580153465271
Epoch 800, Loss: 0.5213744044303894, MSE Loss: 0.16880030930042267, ACC loss: 0.8739484548568726
Epoch 900, Loss: 0.5041570663452148, MSE Loss: 0.16544358432292938, ACC loss: 0.8428705334663391
Epoch 1000, Loss: 0.48819148540496826, MSE Loss: 0.16262482106685638, ACC loss: 0.813758134841919
2023-12-20 00:10:14 INFO     		| Interval 1 (training the model) time 40.06317 s
2023-12-20 00:10:14 INFO     			Refinement at level 3 completed.
2023-12-20 00:10:14 INFO     			Refinement at level 2 completed.
2023-12-20 00:10:14 INFO     			Refinement at level 1 completed.
2023-12-20 00:10:14 INFO     		| Interval 2 (refinement) time 0.05295 s
2023-12-20 00:10:14 INFO     	| Interval 2 (refinement training and applying) time 40.11782 s
2023-12-20 00:10:14 INFO     | Time for this section (main program): 52.50096 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.060651779174805
The empirical coverage with aps is: 0.9014405012130737
The empirical efficiency with daps is: 5.0856709480285645
The empirical coverage with daps is: 0.9037149548530579
2023-12-20 00:10:17.225650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:10:26 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.7, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:10:26 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:10:26 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:10:26 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:10:26 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:10:26 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:10:26 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:10:26 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:10:26 INFO     	| Interval 0 (graph coarsening) time 0.24333 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:10:26 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:10:39 INFO     	| Interval 1 (embedding) time 13.19359 s
2023-12-20 00:10:39 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-20 00:10:39 INFO     initial_embed: (1518, 128)
2023-12-20 00:10:39 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6166979074478149, MSE Loss: 0.4334349036216736, ACC loss: 0.695239245891571
Epoch 200, Loss: 0.41415631771087646, MSE Loss: 0.4076824486255646, ACC loss: 0.41693082451820374
Epoch 300, Loss: 0.28116726875305176, MSE Loss: 0.3799678385257721, ACC loss: 0.23882417380809784
Epoch 400, Loss: 0.2737281322479248, MSE Loss: 0.35680562257766724, ACC loss: 0.23812347650527954
Epoch 500, Loss: 0.2083720862865448, MSE Loss: 0.3319995403289795, ACC loss: 0.15538887679576874
Epoch 600, Loss: 0.19903826713562012, MSE Loss: 0.3183959126472473, ACC loss: 0.14788499474525452
Epoch 700, Loss: 0.18260857462882996, MSE Loss: 0.3034557104110718, ACC loss: 0.13081693649291992
Epoch 800, Loss: 0.17868837714195251, MSE Loss: 0.30759212374687195, ACC loss: 0.12344390153884888
Epoch 900, Loss: 0.2920306324958801, MSE Loss: 0.2929898202419281, ACC loss: 0.29161956906318665
Epoch 1000, Loss: 0.13345560431480408, MSE Loss: 0.2857765853404999, ACC loss: 0.06817518919706345
2023-12-20 00:11:21 INFO     		| Interval 1 (training the model) time 41.89822 s
2023-12-20 00:11:21 INFO     			Refinement at level 3 completed.
2023-12-20 00:11:21 INFO     			Refinement at level 2 completed.
2023-12-20 00:11:21 INFO     			Refinement at level 1 completed.
2023-12-20 00:11:21 INFO     		| Interval 2 (refinement) time 0.05963 s
2023-12-20 00:11:21 INFO     	| Interval 2 (refinement training and applying) time 41.96050 s
2023-12-20 00:11:21 INFO     | Time for this section (main program): 55.39741 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.012130260467529
The empirical coverage with aps is: 0.8946171402931213
The empirical efficiency with daps is: 5.009097576141357
The empirical coverage with daps is: 0.8999241590499878
2023-12-20 00:11:24.457328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:11:33 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.7, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:11:33 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:11:33 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:11:33 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:11:33 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:11:33 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:11:33 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:11:33 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:11:33 INFO     	| Interval 0 (graph coarsening) time 0.24118 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:11:33 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:11:46 INFO     	| Interval 1 (embedding) time 12.54100 s
2023-12-20 00:11:46 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:11:46 INFO     initial_embed: (1518, 128)
2023-12-20 00:11:46 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.686680018901825, MSE Loss: 0.37480321526527405, ACC loss: 0.8203415870666504
Epoch 200, Loss: 0.5902388095855713, MSE Loss: 0.32734355330467224, ACC loss: 0.7029082179069519
Epoch 300, Loss: 0.4891289472579956, MSE Loss: 0.29851749539375305, ACC loss: 0.570819616317749
Epoch 400, Loss: 0.414423406124115, MSE Loss: 0.2809373140335083, ACC loss: 0.4716317057609558
Epoch 500, Loss: 0.34871235489845276, MSE Loss: 0.2643146216869354, ACC loss: 0.3848828077316284
Epoch 600, Loss: 0.30520451068878174, MSE Loss: 0.24549365043640137, ACC loss: 0.3307948708534241
Epoch 700, Loss: 0.31890666484832764, MSE Loss: 0.23345565795898438, ACC loss: 0.35552850365638733
Epoch 800, Loss: 0.24325545132160187, MSE Loss: 0.2296612560749054, ACC loss: 0.2490815371274948
Epoch 900, Loss: 0.27946725487709045, MSE Loss: 0.2093891054391861, ACC loss: 0.3095007538795471
Epoch 1000, Loss: 0.2129744589328766, MSE Loss: 0.20740729570388794, ACC loss: 0.21536040306091309
2023-12-20 00:12:28 INFO     		| Interval 1 (training the model) time 42.54003 s
2023-12-20 00:12:28 INFO     			Refinement at level 3 completed.
2023-12-20 00:12:28 INFO     			Refinement at level 2 completed.
2023-12-20 00:12:28 INFO     			Refinement at level 1 completed.
2023-12-20 00:12:28 INFO     		| Interval 2 (refinement) time 0.05880 s
2023-12-20 00:12:28 INFO     	| Interval 2 (refinement training and applying) time 42.60086 s
2023-12-20 00:12:28 INFO     | Time for this section (main program): 55.38304 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 6.0
The empirical coverage with aps is: 1.0
The empirical efficiency with daps is: 6.0
The empirical coverage with daps is: 1.0
2023-12-20 00:12:32.274748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:12:41 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.7, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:12:41 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:12:41 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:12:41 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:12:41 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:12:41 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:12:41 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:12:41 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:12:41 INFO     	| Interval 0 (graph coarsening) time 0.24345 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:12:41 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:12:52 INFO     	| Interval 1 (embedding) time 11.48132 s
2023-12-20 00:12:52 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:12:52 INFO     initial_embed: (1518, 128)
2023-12-20 00:12:52 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7582062482833862, MSE Loss: 0.4336315989494324, ACC loss: 0.8973096609115601
Epoch 200, Loss: 0.6442241072654724, MSE Loss: 0.3727361857891083, ACC loss: 0.760576069355011
Epoch 300, Loss: 0.5819640755653381, MSE Loss: 0.33175334334373474, ACC loss: 0.6891972422599792
Epoch 400, Loss: 0.5342926383018494, MSE Loss: 0.29639744758605957, ACC loss: 0.6362476944923401
Epoch 500, Loss: 0.4660244584083557, MSE Loss: 0.2867688238620758, ACC loss: 0.5428483486175537
Epoch 600, Loss: 0.414387047290802, MSE Loss: 0.29242751002311707, ACC loss: 0.466655433177948
Epoch 700, Loss: 0.3650858998298645, MSE Loss: 0.28510406613349915, ACC loss: 0.39936381578445435
Epoch 800, Loss: 0.3348080813884735, MSE Loss: 0.2726355791091919, ACC loss: 0.36145341396331787
Epoch 900, Loss: 0.3161693513393402, MSE Loss: 0.25616753101348877, ACC loss: 0.34188440442085266
Epoch 1000, Loss: 0.3098962903022766, MSE Loss: 0.2623310387134552, ACC loss: 0.33028140664100647
2023-12-20 00:13:34 INFO     		| Interval 1 (training the model) time 41.85163 s
2023-12-20 00:13:34 INFO     			Refinement at level 3 completed.
2023-12-20 00:13:34 INFO     			Refinement at level 2 completed.
2023-12-20 00:13:34 INFO     			Refinement at level 1 completed.
2023-12-20 00:13:34 INFO     		| Interval 2 (refinement) time 0.05877 s
2023-12-20 00:13:34 INFO     	| Interval 2 (refinement training and applying) time 41.91242 s
2023-12-20 00:13:34 INFO     | Time for this section (main program): 53.63719 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 6.0
The empirical coverage with aps is: 1.0
The empirical efficiency with daps is: 6.0
The empirical coverage with daps is: 1.0
2023-12-20 00:13:37.607704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:13:46 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.7, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:13:46 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:13:46 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:13:46 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:13:46 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:13:46 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:13:46 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:13:46 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:13:46 INFO     	| Interval 0 (graph coarsening) time 0.24269 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:13:46 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:13:57 INFO     	| Interval 1 (embedding) time 10.65914 s
2023-12-20 00:13:57 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:13:57 INFO     initial_embed: (1518, 128)
2023-12-20 00:13:57 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.1095863580703735, MSE Loss: 0.5125181078910828, ACC loss: 1.3654727935791016
Epoch 200, Loss: 0.8976195454597473, MSE Loss: 0.4699608087539673, ACC loss: 1.080901861190796
Epoch 300, Loss: 0.8026708960533142, MSE Loss: 0.4508453607559204, ACC loss: 0.9534533023834229
Epoch 400, Loss: 0.7492520809173584, MSE Loss: 0.42612430453300476, ACC loss: 0.8877354264259338
Epoch 500, Loss: 0.7118799686431885, MSE Loss: 0.40502673387527466, ACC loss: 0.8433884978294373
Epoch 600, Loss: 0.6813597083091736, MSE Loss: 0.39105769991874695, ACC loss: 0.8057748675346375
Epoch 700, Loss: 0.6495453715324402, MSE Loss: 0.38048887252807617, ACC loss: 0.7648553252220154
Epoch 800, Loss: 0.6160715818405151, MSE Loss: 0.3713991940021515, ACC loss: 0.7209312319755554
Epoch 900, Loss: 0.5803778767585754, MSE Loss: 0.36777710914611816, ACC loss: 0.6714925169944763
Epoch 1000, Loss: 0.5375930070877075, MSE Loss: 0.3670434355735779, ACC loss: 0.6106857061386108
2023-12-20 00:14:36 INFO     		| Interval 1 (training the model) time 38.85168 s
2023-12-20 00:14:36 INFO     			Refinement at level 3 completed.
2023-12-20 00:14:36 INFO     			Refinement at level 2 completed.
2023-12-20 00:14:36 INFO     			Refinement at level 1 completed.
2023-12-20 00:14:36 INFO     		| Interval 2 (refinement) time 0.04928 s
2023-12-20 00:14:36 INFO     	| Interval 2 (refinement training and applying) time 38.90261 s
2023-12-20 00:14:36 INFO     | Time for this section (main program): 49.80445 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.013646602630615
The empirical coverage with aps is: 0.9021986126899719
The empirical efficiency with daps is: 4.951478481292725
The empirical coverage with daps is: 0.8915845155715942
2023-12-20 00:14:39.184074: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:14:47 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.9, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:14:48 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:14:48 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:14:48 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:14:48 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:14:48 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:14:48 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:14:48 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:14:48 INFO     	| Interval 0 (graph coarsening) time 0.24240 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:14:48 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:15:00 INFO     	| Interval 1 (embedding) time 12.14676 s
2023-12-20 00:15:00 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:15:00 INFO     initial_embed: (1518, 128)
2023-12-20 00:15:00 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6047421097755432, MSE Loss: 0.8721460700035095, ACC loss: 0.5750305652618408
Epoch 200, Loss: 0.34573209285736084, MSE Loss: 0.8381341099739075, ACC loss: 0.2910207509994507
Epoch 300, Loss: 0.24750833213329315, MSE Loss: 0.7881416082382202, ACC loss: 0.18743796646595
Epoch 400, Loss: 0.28892403841018677, MSE Loss: 0.7613266706466675, ACC loss: 0.23643484711647034
Epoch 500, Loss: 0.17380192875862122, MSE Loss: 0.7085568308830261, ACC loss: 0.114384725689888
Epoch 600, Loss: 0.16014209389686584, MSE Loss: 0.6760406494140625, ACC loss: 0.10282002389431
Epoch 700, Loss: 0.16758912801742554, MSE Loss: 0.6576107740402222, ACC loss: 0.11314227432012558
Epoch 800, Loss: 0.13771258294582367, MSE Loss: 0.6477119326591492, ACC loss: 0.08104598522186279
Epoch 900, Loss: 0.1380695402622223, MSE Loss: 0.6524447798728943, ACC loss: 0.08091674000024796
Epoch 1000, Loss: 0.10509300231933594, MSE Loss: 0.6167751550674438, ACC loss: 0.04823943227529526
2023-12-20 00:15:43 INFO     		| Interval 1 (training the model) time 42.64876 s
2023-12-20 00:15:43 INFO     			Refinement at level 3 completed.
2023-12-20 00:15:43 INFO     			Refinement at level 2 completed.
2023-12-20 00:15:43 INFO     			Refinement at level 1 completed.
2023-12-20 00:15:43 INFO     		| Interval 2 (refinement) time 0.05554 s
2023-12-20 00:15:43 INFO     	| Interval 2 (refinement training and applying) time 42.70642 s
2023-12-20 00:15:43 INFO     | Time for this section (main program): 55.09558 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.301743507385254
The empirical coverage with aps is: 0.9325246214866638
The empirical efficiency with daps is: 4.950720310211182
The empirical coverage with daps is: 0.9021986126899719
2023-12-20 00:15:46.481808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:15:55 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.9, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:15:55 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:15:55 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:15:55 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:15:55 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:15:55 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:15:55 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:15:55 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:15:55 INFO     	| Interval 0 (graph coarsening) time 0.24117 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:15:55 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:16:08 INFO     	| Interval 1 (embedding) time 12.25567 s
2023-12-20 00:16:08 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:16:08 INFO     initial_embed: (1518, 128)
2023-12-20 00:16:08 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7143068313598633, MSE Loss: 0.8333501219749451, ACC loss: 0.701079785823822
Epoch 200, Loss: 0.6182841062545776, MSE Loss: 0.7653486728668213, ACC loss: 0.6019436120986938
Epoch 300, Loss: 0.5392770171165466, MSE Loss: 0.7697330713272095, ACC loss: 0.513670802116394
Epoch 400, Loss: 0.45548248291015625, MSE Loss: 0.7695482969284058, ACC loss: 0.4205862879753113
Epoch 500, Loss: 0.3723488748073578, MSE Loss: 0.7621117234230042, ACC loss: 0.32904189825057983
Epoch 600, Loss: 0.3416401743888855, MSE Loss: 0.7464028596878052, ACC loss: 0.29666656255722046
Epoch 700, Loss: 0.2516212463378906, MSE Loss: 0.7296145558357239, ACC loss: 0.19851088523864746
Epoch 800, Loss: 0.22531503438949585, MSE Loss: 0.7085621953010559, ACC loss: 0.1716209053993225
Epoch 900, Loss: 0.22232547402381897, MSE Loss: 0.7060438394546509, ACC loss: 0.16857899725437164
Epoch 1000, Loss: 0.18505866825580597, MSE Loss: 0.6888324022293091, ACC loss: 0.1290838122367859
2023-12-20 00:16:49 INFO     		| Interval 1 (training the model) time 41.27814 s
2023-12-20 00:16:49 INFO     			Refinement at level 3 completed.
2023-12-20 00:16:49 INFO     			Refinement at level 2 completed.
2023-12-20 00:16:49 INFO     			Refinement at level 1 completed.
2023-12-20 00:16:49 INFO     		| Interval 2 (refinement) time 0.05642 s
2023-12-20 00:16:49 INFO     	| Interval 2 (refinement training and applying) time 41.33644 s
2023-12-20 00:16:49 INFO     | Time for this section (main program): 53.83327 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.171341896057129
The empirical coverage with aps is: 0.9006823301315308
The empirical efficiency with daps is: 5.321455478668213
The empirical coverage with daps is: 0.919636070728302
2023-12-20 00:16:52.050894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:17:00 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.9, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:17:00 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:17:00 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:17:01 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:17:01 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:17:01 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:17:01 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:17:01 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:17:01 INFO     	| Interval 0 (graph coarsening) time 0.24294 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:17:01 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:17:12 INFO     	| Interval 1 (embedding) time 11.65547 s
2023-12-20 00:17:12 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:17:12 INFO     initial_embed: (1518, 128)
2023-12-20 00:17:12 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.8318060040473938, MSE Loss: 0.8936250805854797, ACC loss: 0.8249372243881226
Epoch 200, Loss: 0.7387330532073975, MSE Loss: 0.8053202033042908, ACC loss: 0.7313345074653625
Epoch 300, Loss: 0.6944180130958557, MSE Loss: 0.7686060070991516, ACC loss: 0.6861749291419983
Epoch 400, Loss: 0.664145290851593, MSE Loss: 0.7602218985557556, ACC loss: 0.6534700989723206
Epoch 500, Loss: 0.6392726898193359, MSE Loss: 0.7433568835258484, ACC loss: 0.6277077794075012
Epoch 600, Loss: 0.6052086353302002, MSE Loss: 0.7361021637916565, ACC loss: 0.5906649231910706
Epoch 700, Loss: 0.5968618392944336, MSE Loss: 0.7244689464569092, ACC loss: 0.5826833248138428
Epoch 800, Loss: 0.561423122882843, MSE Loss: 0.7198158502578735, ACC loss: 0.5438239574432373
Epoch 900, Loss: 0.5341677665710449, MSE Loss: 0.6957498788833618, ACC loss: 0.5162141919136047
Epoch 1000, Loss: 0.477710485458374, MSE Loss: 0.7249804139137268, ACC loss: 0.450236052274704
2023-12-20 00:17:55 INFO     		| Interval 1 (training the model) time 42.56393 s
2023-12-20 00:17:55 INFO     			Refinement at level 3 completed.
2023-12-20 00:17:55 INFO     			Refinement at level 2 completed.
2023-12-20 00:17:55 INFO     			Refinement at level 1 completed.
2023-12-20 00:17:55 INFO     		| Interval 2 (refinement) time 0.06087 s
2023-12-20 00:17:55 INFO     	| Interval 2 (refinement training and applying) time 42.62688 s
2023-12-20 00:17:55 INFO     | Time for this section (main program): 54.52529 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.111448287963867
The empirical coverage with aps is: 0.9021986126899719
The empirical efficiency with daps is: 5.086429119110107
The empirical coverage with daps is: 0.9075056910514832
2023-12-20 00:17:58.179099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:18:07 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.9, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:18:07 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:18:07 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:18:07 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:18:07 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:18:07 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:18:07 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:18:07 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:18:07 INFO     	| Interval 0 (graph coarsening) time 0.24095 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:18:07 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:18:19 INFO     	| Interval 1 (embedding) time 12.27613 s
2023-12-20 00:18:19 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-20 00:18:19 INFO     initial_embed: (1518, 128)
2023-12-20 00:18:19 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.1430104970932007, MSE Loss: 1.0776457786560059, ACC loss: 1.1502732038497925
Epoch 200, Loss: 0.9078002572059631, MSE Loss: 1.1211153268814087, ACC loss: 0.8840986490249634
Epoch 300, Loss: 0.8238793611526489, MSE Loss: 1.0754029750823975, ACC loss: 0.7959322929382324
Epoch 400, Loss: 0.7958383560180664, MSE Loss: 1.0284836292266846, ACC loss: 0.7699888944625854
Epoch 500, Loss: 0.7639398574829102, MSE Loss: 0.9479129910469055, ACC loss: 0.7434984445571899
Epoch 600, Loss: 0.7272709608078003, MSE Loss: 0.9455865621566772, ACC loss: 0.7030137181282043
Epoch 700, Loss: 0.7082201838493347, MSE Loss: 0.921981692314148, ACC loss: 0.6844689249992371
Epoch 800, Loss: 0.6993436217308044, MSE Loss: 0.897959291934967, ACC loss: 0.6772751808166504
Epoch 900, Loss: 0.678367018699646, MSE Loss: 0.8679357767105103, ACC loss: 0.6573038101196289
Epoch 1000, Loss: 0.6727012395858765, MSE Loss: 0.8690902590751648, ACC loss: 0.6508802175521851
2023-12-20 00:19:01 INFO     		| Interval 1 (training the model) time 41.93434 s
2023-12-20 00:19:01 INFO     			Refinement at level 3 completed.
2023-12-20 00:19:01 INFO     			Refinement at level 2 completed.
2023-12-20 00:19:01 INFO     			Refinement at level 1 completed.
2023-12-20 00:19:01 INFO     		| Interval 2 (refinement) time 0.05676 s
2023-12-20 00:19:01 INFO     	| Interval 2 (refinement training and applying) time 41.99375 s
2023-12-20 00:19:01 INFO     | Time for this section (main program): 54.51083 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.2736921310424805
The empirical coverage with aps is: 0.9249431490898132
The empirical efficiency with daps is: 5.170583724975586
The empirical coverage with daps is: 0.9112964272499084
2023-12-20 00:19:04.602945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:19:13 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.99, learning_rate=0.02, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:19:13 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:19:13 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:19:13 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:19:13 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:19:13 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:19:13 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:19:13 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:19:13 INFO     	| Interval 0 (graph coarsening) time 0.24387 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:19:13 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:19:24 INFO     	| Interval 1 (embedding) time 11.15793 s
2023-12-20 00:19:24 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:19:24 INFO     initial_embed: (1518, 128)
2023-12-20 00:19:24 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.6190353035926819, MSE Loss: 1.5366934537887573, ACC loss: 0.6097660660743713
Epoch 200, Loss: 0.3668813705444336, MSE Loss: 1.5789555311203003, ACC loss: 0.3546381890773773
Epoch 300, Loss: 0.1991318017244339, MSE Loss: 1.6625416278839111, ACC loss: 0.1843498796224594
Epoch 400, Loss: 0.13963715732097626, MSE Loss: 1.661346197128296, ACC loss: 0.1242663562297821
Epoch 500, Loss: 0.10711932182312012, MSE Loss: 1.6571956872940063, ACC loss: 0.09146198630332947
Epoch 600, Loss: 0.09059996902942657, MSE Loss: 1.627622127532959, ACC loss: 0.07507448643445969
Epoch 700, Loss: 0.07582047581672668, MSE Loss: 1.6384555101394653, ACC loss: 0.06003628671169281
Epoch 800, Loss: 0.051459021866321564, MSE Loss: 1.6275579929351807, ACC loss: 0.03553883358836174
Epoch 900, Loss: 0.651008665561676, MSE Loss: 1.667015790939331, ACC loss: 0.6407459378242493
Epoch 1000, Loss: 0.0490800105035305, MSE Loss: 1.6476073265075684, ACC loss: 0.03293326869606972
2023-12-20 00:20:06 INFO     		| Interval 1 (training the model) time 41.38797 s
2023-12-20 00:20:06 INFO     			Refinement at level 3 completed.
2023-12-20 00:20:06 INFO     			Refinement at level 2 completed.
2023-12-20 00:20:06 INFO     			Refinement at level 1 completed.
2023-12-20 00:20:06 INFO     		| Interval 2 (refinement) time 0.05291 s
2023-12-20 00:20:06 INFO     	| Interval 2 (refinement training and applying) time 41.44250 s
2023-12-20 00:20:06 INFO     | Time for this section (main program): 52.84429 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 6.0
The empirical coverage with aps is: 1.0
The empirical efficiency with daps is: 5.585291862487793
The empirical coverage with daps is: 0.9302501678466797
2023-12-20 00:20:08.883107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:20:17 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.99, learning_rate=0.01, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:20:17 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:20:17 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:20:17 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:20:18 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:20:18 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:20:18 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:20:18 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:20:18 INFO     	| Interval 0 (graph coarsening) time 0.24224 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:20:18 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:20:28 INFO     	| Interval 1 (embedding) time 10.82873 s
2023-12-20 00:20:28 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:20:28 INFO     initial_embed: (1518, 128)
2023-12-20 00:20:28 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7466274499893188, MSE Loss: 1.4245781898498535, ACC loss: 0.7397794127464294
Epoch 200, Loss: 0.67055743932724, MSE Loss: 1.3762608766555786, ACC loss: 0.6634291410446167
Epoch 300, Loss: 0.552794337272644, MSE Loss: 1.4140119552612305, ACC loss: 0.5440951585769653
Epoch 400, Loss: 0.45241573452949524, MSE Loss: 1.4656826257705688, ACC loss: 0.44218072295188904
Epoch 500, Loss: 0.3496469557285309, MSE Loss: 1.499678611755371, ACC loss: 0.33803045749664307
Epoch 600, Loss: 0.2816360294818878, MSE Loss: 1.552355170249939, ACC loss: 0.2688004672527313
Epoch 700, Loss: 0.241331085562706, MSE Loss: 1.5572165250778198, ACC loss: 0.22803930938243866
Epoch 800, Loss: 0.19270813465118408, MSE Loss: 1.5935910940170288, ACC loss: 0.17855779826641083
Epoch 900, Loss: 0.17059844732284546, MSE Loss: 1.5883506536483765, ACC loss: 0.15627771615982056
Epoch 1000, Loss: 0.2082211971282959, MSE Loss: 1.4938089847564697, ACC loss: 0.1952354609966278
2023-12-20 00:21:12 INFO     		| Interval 1 (training the model) time 43.74704 s
2023-12-20 00:21:12 INFO     			Refinement at level 3 completed.
2023-12-20 00:21:12 INFO     			Refinement at level 2 completed.
2023-12-20 00:21:12 INFO     			Refinement at level 1 completed.
2023-12-20 00:21:12 INFO     		| Interval 2 (refinement) time 0.05270 s
2023-12-20 00:21:12 INFO     	| Interval 2 (refinement training and applying) time 43.80135 s
2023-12-20 00:21:12 INFO     | Time for this section (main program): 54.87232 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.284306526184082
The empirical coverage with aps is: 0.904473066329956
The empirical efficiency with daps is: 5.398787021636963
The empirical coverage with daps is: 0.9158453345298767
2023-12-20 00:21:15.489587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:21:24 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.99, learning_rate=0.005, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:21:24 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:21:24 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:21:24 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:21:24 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:21:24 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:21:24 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:21:24 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:21:24 INFO     	| Interval 0 (graph coarsening) time 0.24391 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:21:24 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:21:35 INFO     	| Interval 1 (embedding) time 11.12087 s
2023-12-20 00:21:35 INFO     		| Interval 0 (double-base embedding) time 0.00002 s
2023-12-20 00:21:35 INFO     initial_embed: (1518, 128)
2023-12-20 00:21:35 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 0.7811480164527893, MSE Loss: 1.5247161388397217, ACC loss: 0.7736372351646423
Epoch 200, Loss: 0.6753478050231934, MSE Loss: 1.475016713142395, ACC loss: 0.667270302772522
Epoch 300, Loss: 0.6350992918014526, MSE Loss: 1.4733599424362183, ACC loss: 0.6266319751739502
Epoch 400, Loss: 0.6924483776092529, MSE Loss: 1.2631951570510864, ACC loss: 0.6866832375526428
Epoch 500, Loss: 0.581508994102478, MSE Loss: 1.4698841571807861, ACC loss: 0.5725354552268982
Epoch 600, Loss: 0.5498672127723694, MSE Loss: 1.46006178855896, ACC loss: 0.5406733155250549
Epoch 700, Loss: 0.5295190215110779, MSE Loss: 1.4464396238327026, ACC loss: 0.5202572345733643
Epoch 800, Loss: 0.537630021572113, MSE Loss: 1.4890638589859009, ACC loss: 0.5280196070671082
Epoch 900, Loss: 0.47482597827911377, MSE Loss: 1.4940143823623657, ACC loss: 0.46453115344047546
Epoch 1000, Loss: 0.4406299889087677, MSE Loss: 1.4816659688949585, ACC loss: 0.4301144480705261
2023-12-20 00:22:16 INFO     		| Interval 1 (training the model) time 40.57769 s
2023-12-20 00:22:16 INFO     			Refinement at level 3 completed.
2023-12-20 00:22:16 INFO     			Refinement at level 2 completed.
2023-12-20 00:22:16 INFO     			Refinement at level 1 completed.
2023-12-20 00:22:16 INFO     		| Interval 2 (refinement) time 0.05625 s
2023-12-20 00:22:16 INFO     	| Interval 2 (refinement training and applying) time 40.63553 s
2023-12-20 00:22:16 INFO     | Time for this section (main program): 52.00031 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.264594554901123
The empirical coverage with aps is: 0.8824867606163025
The empirical efficiency with daps is: 5.282032012939453
The empirical coverage with daps is: 0.890826404094696
2023-12-20 00:22:19.135424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-20 00:22:28 INFO     Namespace(alpha=0.1, baseline=False, basic_embed='node2vec', coarsen_level=3, data='citeseer', diffusion_param=0.1, double_base=False, embed_dim=128, epoch=1000, jobid=92499, lambda_fl=0.99, learning_rate=0.001, no_eval=False, only_eval=False, refine_type='conf', report_epoch=100, seed=20, self_weight=0.05, store_embed=False, task='nc', train_ratio=0.3, use_aps_epsilon=True, valid=False, valid_epoch=20, workers=128)
2023-12-20 00:22:28 INFO     # groups have perfect jaccard idx (1.0): 109
2023-12-20 00:22:28 INFO     # groups have perfect jaccard idx (1.0): 24
2023-12-20 00:22:28 INFO     # groups have perfect jaccard idx (1.0): 17
2023-12-20 00:22:28 INFO     Level 0 --- # nodes: 3312 , # edges: 9196
2023-12-20 00:22:28 INFO     Level 1 --- # nodes: 2199 , # edges: 7030
2023-12-20 00:22:28 INFO     Level 2 --- # nodes: 1746 , # edges: 5494
2023-12-20 00:22:28 INFO     Level 3 --- # nodes: 1518 , # edges: 4372
2023-12-20 00:22:28 INFO     	| Interval 0 (graph coarsening) time 0.24682 s
walk_path ./base_embed_methods/FairWalk/tmp/node2vec_1518.walk
2023-12-20 00:22:28 INFO     ./base_embed_methods/FairWalk/fast-random-walk/walk --if=./base_embed_methods/FairWalk/tmp/node2vec_1518.edgelist --of=./base_embed_methods/FairWalk/tmp/node2vec_1518.walk --length=80 --walks=20 -w
training done
2023-12-20 00:22:39 INFO     	| Interval 1 (embedding) time 11.29298 s
2023-12-20 00:22:39 INFO     		| Interval 0 (double-base embedding) time 0.00001 s
2023-12-20 00:22:39 INFO     initial_embed: (1518, 128)
2023-12-20 00:22:39 INFO     fine_embed: (1518, 128)
Epoch 100, Loss: 1.0812395811080933, MSE Loss: 1.7240480184555054, ACC loss: 1.0747464895248413
Epoch 200, Loss: 0.8614848256111145, MSE Loss: 1.758104681968689, ACC loss: 0.8524280786514282
Epoch 300, Loss: 0.786586582660675, MSE Loss: 1.7130236625671387, ACC loss: 0.777228593826294
Epoch 400, Loss: 0.7357459664344788, MSE Loss: 1.711111307144165, ACC loss: 0.7258937954902649
Epoch 500, Loss: 0.7047424912452698, MSE Loss: 1.6852924823760986, ACC loss: 0.6948379278182983
Epoch 600, Loss: 0.6943926215171814, MSE Loss: 1.6400306224822998, ACC loss: 0.684840738773346
Epoch 700, Loss: 0.6843170523643494, MSE Loss: 1.6486761569976807, ACC loss: 0.6745760440826416
Epoch 800, Loss: 0.6775048971176147, MSE Loss: 1.470758080482483, ACC loss: 0.669492244720459
Epoch 900, Loss: 0.6312313079833984, MSE Loss: 1.6033170223236084, ACC loss: 0.6214122772216797
Epoch 1000, Loss: 0.6211989521980286, MSE Loss: 1.5953549146652222, ACC loss: 0.6113590002059937
2023-12-20 00:23:23 INFO     		| Interval 1 (training the model) time 43.87465 s
2023-12-20 00:23:23 INFO     			Refinement at level 3 completed.
2023-12-20 00:23:23 INFO     			Refinement at level 2 completed.
2023-12-20 00:23:23 INFO     			Refinement at level 1 completed.
2023-12-20 00:23:23 INFO     		| Interval 2 (refinement) time 0.06288 s
2023-12-20 00:23:23 INFO     	| Interval 2 (refinement training and applying) time 43.93884 s
2023-12-20 00:23:23 INFO     | Time for this section (main program): 55.47864 s
/home/he.1773/workplace/confMILE/transformations.py:27: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1702400431970/work/torch/csrc/utils/tensor_new.cpp:605.)
  A = torch.sparse.FloatTensor(
The empirical efficiency with aps is: 5.148597240447998
The empirical coverage with aps is: 0.8688400387763977
The empirical efficiency with daps is: 5.226686954498291
The empirical coverage with daps is: 0.8847611546516418
